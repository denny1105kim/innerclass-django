from __future__ import annotations

import re
import time
from dataclasses import dataclass
from datetime import datetime, timezone as dt_timezone
from typing import Optional
from urllib.parse import urljoin, urlparse, urlsplit, urlunsplit

import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from django.db import transaction
from django.utils import timezone

from news.models import NewsArticle, NewsSector, NewsMarket
from news.services.embedding_news import embed_passages
from news.services.news_queue import enqueue_article_for_classify


@dataclass(frozen=True)
class GenericSource:
    name: str
    urls: list[str]
    base_url: str
    # ÏÑπÏÖò/Î¶¨Ïä§Ìä∏ ÌéòÏù¥ÏßÄÏóêÏÑú "Í∏∞ÏÇ¨ ÎßÅÌÅ¨"Î°ú Ïù∏Ï†ïÌï† Ìå®ÌÑ¥(ÌïòÎÇòÎùºÎèÑ Îß§Ïπ≠ÎêòÎ©¥ ÌõÑÎ≥¥)
    must_contain_any: tuple[str, ...] = ()
    # Ï†úÏô∏ Ìå®ÌÑ¥(ÌïòÎÇòÎùºÎèÑ Îß§Ïπ≠ÎêòÎ©¥ Ï†úÏô∏)
    exclude_any: tuple[str, ...] = ()
    # ÎßÅÌÅ¨ ÎèÑÎ©îÏù∏ Í∞ïÏ†ú(Îπà Í∞íÏù¥Î©¥ base_url Í∏∞Î∞ò)
    domain_prefix: str = ""
    # URLÏù¥ "Í∏∞ÏÇ¨" ÌòïÌÉúÏù∏ÏßÄ Ï†ïÍ∑úÏãùÏúºÎ°ú Í∞ïÏ†ú (ÌïòÎÇòÎùºÎèÑ Îß§Ïπ≠ÎêòÎ©¥ ÌÜµÍ≥º)
    must_match_regex: tuple[str, ...] = ()


class Command(BaseCommand):
    help = (
        "Íµ≠ÎÇ¥ Ï£ºÏöî Í≤ΩÏ†ú/ÏÇ∞ÏóÖ Îâ¥Ïä§Î•º ÏÑπÏÖò URL Í∏∞Î∞òÏúºÎ°ú ÌÅ¨Î°§ÎßÅÌïòÏó¨ DB Ï†ÄÏû•Ìï©ÎãàÎã§. "
        "(Î°úÏª¨ ÏûÑÎ≤†Îî© + Ïù¥ÎØ∏ÏßÄ ÌïÑÌÑ∞ÎßÅ/Í≤ÄÏ¶ù(ÏôÑÌôî) + Redis ÌÅê enqueue; LLM Î∂ÑÎ•òÎäî Î≥ÑÎèÑ workerÍ∞Ä ÏàòÌñâ)"
    )

    # ÏÑπÏÖò(URL) 1Í∞úÎãπ ÏµúÎåÄ Ïã†Í∑ú Ï†ÄÏû•
    MAX_PER_SOURCE = 500

    # ÏöîÏ≤≠ Í∞ÑÍ≤©(Í≥ºÎèÑÌïú Ìä∏ÎûòÌîΩ Î∞©ÏßÄ)
    SLEEP_BETWEEN_ITEMS = 0.08
    SLEEP_BETWEEN_SECTIONS = 0.25

    # ------------------------------------------------------
    # Image filtering / validation (ÏôÑÌôî)
    # ------------------------------------------------------
    BAD_IMAGE_PATTERNS = [
        r"placeholder",
        r"default",
        r"no[-_ ]?image",
        r"no[-_ ]?photo",
        r"image[-_ ]?not[-_ ]?available",
        r"not[-_ ]?found",
        r"spacer",
        r"sprite",
        r"blank",
        r"transparent",
        r"1x1",
        r"pixel",
        r"favicon",
    ]
    BAD_PATH_EXT = (".html", ".htm", ".php", ".aspx", ".jsp")

    VALIDATE_IMAGE_HEAD = True
    IMAGE_HEAD_TIMEOUT = 4
    MAX_IMAGE_BYTES = 10 * 1024 * 1024  # 10MB

    # ------------------------------------------------------
    # URL / Title filtering Í∞ïÌôî (Î©îÏù∏/ÏÑπÏÖò/Î©îÎâ¥ ÎßÅÌÅ¨ Ï∞®Îã®)
    # ------------------------------------------------------
    ARTICLE_DATE_RE = re.compile(r"/20\d{2}/\d{2}/\d{2}/")
    ARTICLE_HTMLDIR_RE = re.compile(r"/site/data/html_dir/")

    # "Í∏∞ÏÇ¨"Ïùº ÌôïÎ•†Ïù¥ ÎÜíÏùÄ Í≥µÌÜµ Ìå®ÌÑ¥
    ARTICLE_LIKELY_RE_LIST = [
        ARTICLE_DATE_RE,
        ARTICLE_HTMLDIR_RE,
        re.compile(r"/article/"),
        re.compile(r"/news/view"),
        re.compile(r"/news/read"),
        re.compile(r"/news/articleView\.html"),
        re.compile(r"/view\.php"),
        re.compile(r"/view/"),
        re.compile(r"/mtview\.php"),
        re.compile(r"/NewsView/"),
        re.compile(r"/news/view/"),
        re.compile(r"/news/article/"),
        re.compile(r"/BP\?command=(mobile_view|article_view)"),
    ]

    # "Í∏∞ÏÇ¨ ÏïÑÎãå" ÌóàÎ∏å/Ïπ¥ÌÖåÍ≥†Î¶¨/Î©îÎâ¥ URL Ìå®ÌÑ¥
    NON_ARTICLE_URL_RE_LIST = [
        re.compile(r"/(search|login|member|subscription|subscribe|mypage)(/|$)"),
        re.compile(r"/(photo|video|vod|podcast|gallery)(/|$)"),
        re.compile(r"/(section|category|categories|tag|tags|topic|topics)(/|$)"),
        re.compile(r"/(company|about|notice|event|press|policy)(/|$)"),
        re.compile(r"/news/?$"),
        re.compile(r"/news/section"),
        re.compile(r"/NewsList/"),
        re.compile(r"/Stock/?$"),
        re.compile(r"/economy/?$"),
        re.compile(r"/industry/?$"),
        re.compile(r"/stock/?$"),
        re.compile(r"/it/?$"),
        re.compile(r"/weeklybiz/?$"),
        re.compile(r"/(lists|list)\b"),
    ]

    MENU_TITLE_KEYWORDS = (
        "Î∞îÎ°úÍ∞ÄÍ∏∞",
        "Í≥µÏßÄ",
        "ÏïåÎ¶º",
        "ÎçîÎ≥¥Í∏∞",
        "Ï†ÑÏ≤¥Î≥¥Í∏∞",
        "Ï†ÑÏ≤¥",
        "Í≤ÄÏÉâ",
        "Î°úÍ∑∏Ïù∏",
        "Íµ¨ÎèÖ",
        "Î©§Î≤ÑÏã≠",
        "ÌöåÏõê",
        "Î©îÎâ¥",
        "ÏÑπÏÖò",
        "Ïπ¥ÌÖåÍ≥†Î¶¨",
        "ÎùºÏù¥Î∏å",
        "ÏòÅÏÉÅ",
        "Ìè¨ÌÜ†",
        "ÏÇ¨ÏßÑ",
        "Í∞§Îü¨Î¶¨",
        "Í∏∞Ìöç",
        "ÏπºÎüº",
        "ÏÇ¨ÏÑ§",
        "Ïò§ÌîºÎãàÏñ∏",
        "Í∏∞ÏûêÏùò",
        "ÌäπÌååÏõê",
        "Ï†ÑÎ¨∏Í∞Ä",
        "ÏãúÍ∞Å",
        "Î∞©ÏÜ°",
        "ÎØ∏ÎîîÏñ¥",
        "IT¬∑Ïù∏ÌÑ∞ÎÑ∑",
        "Ï†ÑÍ∏∞¬∑Ï†ÑÏûê¬∑ÌÜµÏã†",
        "ÊúùÈÆÆÏπºÎüº",
        "The Column",
        # Î¨∏Ï†úÎ°ú Ï†úÏãúÎêú Í≤ÉÎì§
        "enter on news",
        "ai Ïä§ÌäúÎîîÏò§",
        "the biz times",
        "Ïä§ÌéòÏÖúÏóêÎîîÏÖò",
        "special edition",
        "special",
        "edition",
        "Desk pick",
    )

    MENU_TITLE_SHORT_RE = re.compile(r"^(Íµ≠ÎÇ¥|Ìï¥Ïô∏|Í≤ΩÏ†ú|ÏÇ∞ÏóÖ|Ï¶ùÍ∂å|Ï†ïÏπò|ÏÇ¨Ìöå|Íµ≠Ï†ú|Î¨∏Ìôî|Ïä§Ìè¨Ï∏†|Ïó∞Ïòà|IT|ÌÖåÌÅ¨)$")

    BAD_HREF_PREFIXES = ("javascript:", "mailto:", "tel:")
    BAD_HREF_EXACT = ("#", "")

    # ÌÉÄÏù¥ÌãÄ ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞Ïö©
    TITLE_DATE_TIME_RE = re.compile(r"(20\d{2}[-./]\d{2}[-./]\d{2})(\s+\d{2}:\d{2})?")
    TITLE_ONLY_PIPES_RE = re.compile(r"^[\s\|\-‚Äì‚Äî¬∑‚Ä¢\u00b7]+$")
    TITLE_ARROW_RE = re.compile(r"[‚ùØ‚Ä∫¬ª>]+")
    TITLE_MULTI_SPACE_RE = re.compile(r"\s+")
    TITLE_CATEGORY_NOISE_RE = re.compile(r"^(Íµ≠ÎÇ¥|Ìï¥Ïô∏)\s*$")

    # ------------------------------------------------------
    # Section URLs
    # ------------------------------------------------------
    NAVER_URLS = ["https://finance.naver.com/news/"]

    HANKYUNG_URLS = [
        "https://www.hankyung.com/industry",
        "https://www.hankyung.com/industry/semicon-electronics",
        "https://www.hankyung.com/industry/auto-battery",
        "https://www.hankyung.com/industry/ship-marine",
        "https://www.hankyung.com/industry/steel-chemical",
        "https://www.hankyung.com/industry/robot-future",
    ]

    MK_URLS = [
        "https://www.mk.co.kr/news/economy/",
        "https://www.mk.co.kr/news/business/",
        "https://www.mk.co.kr/news/business/electronic/",
        "https://www.mk.co.kr/news/business/automobile/",
        "https://www.mk.co.kr/news/business/chemical/",
        "https://www.mk.co.kr/news/it/",
        "https://www.mk.co.kr/news/it/internet/",
        "https://www.mk.co.kr/news/it/science/",
        "https://www.mk.co.kr/news/stock/",
        "https://www.mk.co.kr/news/stock/business-information/",
    ]

    YONHAP_URLS = [
        "https://www.yna.co.kr/economy/index?site=navi_economy_depth01",
        "https://www.yna.co.kr/industry/all",
        "https://www.yna.co.kr/industry/industrial-enterprise",
        "https://www.yna.co.kr/industry/electronics",
        "https://www.yna.co.kr/industry/heavy-chemistry",
        "https://www.yna.co.kr/industry/automobile",
        "https://www.yna.co.kr/industry/energy-resource",
        "https://www.yna.co.kr/industry/technology-science",
        "https://www.yna.co.kr/industry/bioindustry-health",
        "https://www.yna.co.kr/market-plus/all",
        "https://www.yna.co.kr/market-plus/domestic-stock",
    ]

    CHOSUN_URLS = [
        "https://www.chosun.com/economy/tech_it/",
        "https://www.chosun.com/economy/auto/",
        "https://www.chosun.com/economy/real_estate/",
        "https://www.chosun.com/economy/science/",
        "https://www.chosun.com/weeklybiz/",
    ]

    # ------------------------------------------------------
    # Generic sources
    # ------------------------------------------------------
    GENERIC_SOURCES: list[GenericSource] = [
        GenericSource(
            name="DaumFinance",
            urls=["https://finance.daum.net/news"],
            base_url="https://finance.daum.net",
            must_contain_any=("/news/",),
            exclude_any=("/news/home",),
            must_match_regex=(r"/news/[^/]{2,}$",),
        ),
        GenericSource(
            name="Sedaily",
            urls=["https://www.sedaily.com/NewsList/GD", "https://www.sedaily.com/Stock"],
            base_url="https://www.sedaily.com",
            must_contain_any=("/NewsView/",),
            must_match_regex=(r"/NewsView/\d+",),
        ),
        GenericSource(
            name="Asiae",
            urls=["https://www.asiae.co.kr/list/economy"],
            base_url="https://www.asiae.co.kr",
            must_contain_any=("/article/",),
            must_match_regex=(r"/article/\d+",),
        ),
        GenericSource(
            name="Edaily",
            urls=["https://www.edaily.co.kr/news/section/economy"],
            base_url="https://www.edaily.co.kr",
            must_contain_any=("/news/read", "/news/news_detail"),
            must_match_regex=(r"/news/read", r"/news/news_detail"),
        ),
        GenericSource(
            name="Fnnews",
            urls=["https://www.fnnews.com/economy"],
            base_url="https://www.fnnews.com",
            must_contain_any=("/news/",),
            must_match_regex=(r"/news/\d+", r"/news/\w+\d+"),
        ),
        GenericSource(
            name="MoneyToday",
            urls=["https://news.mt.co.kr/newsList.html?type=ent"],
            base_url="https://news.mt.co.kr",
            must_contain_any=("/mtview.php",),
            must_match_regex=(r"/mtview\.php\?no=\d+",),
        ),
        GenericSource(
            name="Etoday",
            urls=["https://www.etoday.co.kr/news/section/subsection?MID=1200"],
            base_url="https://www.etoday.co.kr",
            must_contain_any=("/news/view/",),
            must_match_regex=(r"/news/view/\d+",),
        ),
        GenericSource(
            name="NewsPim",
            urls=["https://www.newspim.com/news/lists/?category=eco"],
            base_url="https://www.newspim.com",
            must_contain_any=("/news/view",),
            must_match_regex=(r"/news/view",),
        ),
        GenericSource(
            name="MoneyS",
            urls=["https://www.moneys.co.kr/"],
            base_url="https://www.moneys.co.kr",
            must_contain_any=("/article/",),
            must_match_regex=(r"/article/\d+",),
        ),
        GenericSource(
            name="ChosunBiz",
            urls=["https://biz.chosun.com/"],
            base_url="https://biz.chosun.com",
            must_contain_any=("/site/data/html_dir/", "/economy/", "/industry/", "/stock/"),
            exclude_any=("/photo/", "/entertainments/", "/video/", "/vod/", "/subscription/", "/member/"),
            must_match_regex=(r"/20\d{2}/\d{2}/\d{2}/", r"/site/data/html_dir/"),
        ),
    ]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": (
                    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                    "(KHTML, like Gecko) Chrome/120 Safari/537.36"
                )
            }
        )

    # -------------------------------
    # Time helpers (UTC normalize)
    # -------------------------------
    def _to_utc(self, dt: Optional[datetime]) -> datetime:
        if not dt:
            now = timezone.now()
            if timezone.is_naive(now):
                now = timezone.make_aware(now, timezone.get_current_timezone())
            return now.astimezone(dt_timezone.utc)

        if timezone.is_naive(dt):
            dt = timezone.make_aware(dt, timezone.get_current_timezone())

        return dt.astimezone(dt_timezone.utc)

    def handle(self, *args, **kwargs):
        self.stdout.write("=========================================")
        self.stdout.write("üì° Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú Í∞ÄÎèô ÏãúÏûë (Î°úÏª¨ ÏûÑÎ≤†Îî© + Redis ÌÅê enqueue)")
        self.stdout.write("- LLM Î∂ÑÎ•ò/Î∂ÑÏÑù: OFF (sector-workerÍ∞Ä Ï≤òÎ¶¨)")
        self.stdout.write(f"- Ïù¥ÎØ∏ÏßÄ ÌïÑÌÑ∞: head_validate={self.VALIDATE_IMAGE_HEAD} (Ïã§Ìå®Ìï¥ÎèÑ Ï†ÄÏû•ÏùÄ ÏßÑÌñâ)")
        self.stdout.write(f"- max_per_section: {self.MAX_PER_SOURCE}")
        self.stdout.write("=========================================")

        total_saved = 0
        total_saved += self.crawl_naver()
        total_saved += self.crawl_yonhap()
        total_saved += self.crawl_hankyung()
        total_saved += self.crawl_mk()
        total_saved += self.crawl_chosun()
        total_saved += self.crawl_generic_sources()

        self.stdout.write("=========================================")
        self.stdout.write(self.style.SUCCESS(f"‚úÖ ÌÜµÌï© ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å. (Ï¥ù Ïã†Í∑ú Ï†ÄÏû•: {total_saved}Í∞ú)"))
        self.stdout.write("=========================================")

    # -------------------------------
    # Utilities
    # -------------------------------
    def _normalize_url(self, url: str) -> str:
        u = (url or "").strip()
        if not u:
            return ""
        try:
            parts = urlsplit(u)
            # query Ïú†ÏßÄ(Í∏∞Í¥Ä/Î≥¥ÎèÑÏûêÎ£å Îì±)
            return urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))
        except Exception:
            return u

    def _normalize_url_noquery(self, url: str) -> str:
        u = (url or "").strip()
        if not u:
            return ""
        try:
            parts = urlsplit(u)
            return urlunsplit((parts.scheme, parts.netloc, parts.path, "", ""))
        except Exception:
            return u

    def _clean_title_text(self, raw: str) -> str:
        """
        ÏÑπÏÖò/Î©îÎâ¥ÏóêÏÑú ÏÑûÏó¨ Îì§Ïñ¥Ïò§Îäî Ïû°ÌÖçÏä§Ìä∏Î•º ÏµúÎåÄÌïú Ï†úÍ±∞Ìï¥ÏÑú
        "Í∏∞ÏÇ¨ Ï†úÎ™©"Îßå ÎÇ®Í∏∞ÎèÑÎ°ù Ï†ïÏ†ú.
        """
        t = (raw or "").strip()
        if not t:
            return ""

        # Ï§ÑÎ∞îÍøà/ÌÉ≠ Îì± Í≥µÎ∞± ÌÜµÏùº
        t = t.replace("\n", " ").replace("\r", " ").replace("\t", " ")
        t = self.TITLE_MULTI_SPACE_RE.sub(" ", t).strip()

        # ÌôîÏÇ¥Ìëú/ÎÑ§ÎπÑ ÌëúÏãù Ï†úÍ±∞
        t = self.TITLE_ARROW_RE.sub("", t).strip()

        # ÎÇ†Ïßú/ÏãúÍ∞Ñ ÌÜ†ÌÅ∞ Ï†úÍ±∞(Î©îÎâ¥Ïóê Î∂ôÎäî ÏºÄÏù¥Ïä§)
        t = self.TITLE_DATE_TIME_RE.sub("", t).strip()

        # Ïñë ÎÅù Íµ¨Î∂ÑÏûê Ï†úÍ±∞
        t = t.strip(" |¬∑‚Ä¢-‚Äì‚Äî>‚Ä∫¬ª‚ùØ")

        t = self.TITLE_MULTI_SPACE_RE.sub(" ", t).strip()
        return t[:500]

    def _normalize_title(self, title: str) -> str:
        t = title or ""
        t = re.sub(r"^[\d\.\s]+", "", t)
        t = self._clean_title_text(t)
        return t[:500]

    def _is_duplicate(self, title: str, url: str) -> bool:
        title_n = self._normalize_title(title)
        url_n = self._normalize_url_noquery(url)

        if title_n and NewsArticle.objects.filter(title=title_n).exists():
            return True
        if url_n and NewsArticle.objects.filter(url=url_n).exists():
            return True
        if title and NewsArticle.objects.filter(title=title).exists():
            return True
        if url and NewsArticle.objects.filter(url=url).exists():
            return True
        return False

    # -------------------------------
    #  ‚ÄúÎ©îÎâ¥/ÏÑπÏÖò ÎßÅÌÅ¨‚Äù ÌåêÎ≥Ñ(Í∞ïÌôî)
    # -------------------------------
    def _looks_like_menu_or_section_title(self, title: str) -> bool:
        t = (title or "").strip()
        if not t:
            return True

        if self.MENU_TITLE_SHORT_RE.match(t):
            return True
        if self.TITLE_CATEGORY_NOISE_RE.match(t):
            return True

        # ÎÑàÎ¨¥ ÏßßÏúºÎ©¥ Í∏∞ÏÇ¨Ïùº ÌôïÎ•† ÎÇÆÏùå
        if len(t) < 8:
            return True

        # Í∏∞Ìò∏Îßå
        if self.TITLE_ONLY_PIPES_RE.match(t):
            return True

        low = t.lower()

        for kw in self.MENU_TITLE_KEYWORDS:
            if kw and (kw.lower() in low):
                return True

        if "¬∑" in t and len(t) <= 16:
            return True

        if "enter on" in low and "news" in low:
            return True

        return False

    def _looks_like_article_url(self, url: str) -> bool:
        u = (url or "").strip()
        if not u:
            return False

        # non-articleÏù¥Î©¥ Ï¶âÏãú Ïª∑
        for rx in self.NON_ARTICLE_URL_RE_LIST:
            if rx.search(u):
                return False

        # Í∞ïÌïú Í∏∞ÏÇ¨ ÏãúÍ∑∏ÎÑê
        if self.ARTICLE_DATE_RE.search(u):
            return True
        if self.ARTICLE_HTMLDIR_RE.search(u):
            return True

        # Í∑∏ Ïô∏Îäî likely Ìå®ÌÑ¥
        for rx in self.ARTICLE_LIKELY_RE_LIST:
            if rx.search(u):
                return True

        return False

    # ----------------------------
    # Image filtering helpers (ÏôÑÌôî)
    # ----------------------------
    def _looks_like_bad_image_url(self, image_url: str) -> bool:
        u = (image_url or "").strip()
        if not u:
            return True
        if not (u.startswith("http://") or u.startswith("https://")):
            return True

        path = urlparse(u).path.lower()
        if path.endswith(self.BAD_PATH_EXT):
            return True

        low = u.lower()
        for pat in self.BAD_IMAGE_PATTERNS:
            if re.search(pat, low):
                return True
        return False

    def _is_real_image_by_head(self, image_url: str) -> bool:
        try:
            r = self.session.head(image_url, timeout=self.IMAGE_HEAD_TIMEOUT, allow_redirects=True)
            if r.status_code >= 400:
                return False

            ctype = (r.headers.get("Content-Type") or "").lower()
            clen = r.headers.get("Content-Length")
            if clen:
                try:
                    if int(clen) > self.MAX_IMAGE_BYTES:
                        return False
                except Exception:
                    pass

            if ctype.startswith("image/"):
                return True

            rg = self.session.get(
                image_url,
                timeout=self.IMAGE_HEAD_TIMEOUT,
                allow_redirects=True,
                stream=True,
                headers={"Range": "bytes=0-2047"},
            )
            if rg.status_code >= 400:
                return False

            ctype2 = (rg.headers.get("Content-Type") or "").lower()
            clen2 = (rg.headers.get("Content-Length") or "").strip()
            if clen2:
                try:
                    if int(clen2) > self.MAX_IMAGE_BYTES:
                        return False
                except Exception:
                    pass

            return ctype2.startswith("image/")
        except Exception:
            return False

    def _pick_valid_image_url(self, image_url: Optional[str]) -> Optional[str]:
        u = (image_url or "").strip()
        if not u:
            return None
        if self._looks_like_bad_image_url(u):
            return None
        if self.VALIDATE_IMAGE_HEAD and not self._is_real_image_by_head(u):
            return None
        return u

    # -------------------------------
    # Detail page helpers (OG + JSON-LD)
    # -------------------------------
    def _fetch_og(self, url: str) -> tuple[Optional[str], Optional[str], Optional[datetime], bool]:
        """
        return: (og_image, og_desc, published_at, is_article_like)
        """
        try:
            res = self.session.get(url, timeout=10)
            if res.status_code >= 400:
                return None, None, None, False

            soup = BeautifulSoup(res.text, "html.parser")

            og_image = None
            og_desc = None
            published_at = None

            m_img = soup.find("meta", property="og:image")
            if m_img and m_img.get("content"):
                og_image = (m_img.get("content") or "").strip()

            m_desc = soup.find("meta", property="og:description")
            if m_desc and m_desc.get("content"):
                og_desc = (m_desc.get("content") or "").strip()

            m_pub = soup.find("meta", property="article:published_time")
            if m_pub and m_pub.get("content"):
                published_at = self._parse_iso_dt(m_pub.get("content"))

            is_article_like = False

            og_type = soup.find("meta", property="og:type")
            if og_type and (og_type.get("content") or "").strip().lower() in ("article", "news", "newsarticle"):
                is_article_like = True

            if not is_article_like:
                for s in soup.find_all("script", attrs={"type": "application/ld+json"})[:10]:
                    txt = (s.get_text() or "").strip()
                    if not txt:
                        continue
                    low = txt.lower()
                    if '"@type"' in low and ("newsarticle" in low or '"article"' in low or '"reportage"' in low):
                        is_article_like = True
                        break

            return og_image, og_desc, published_at, is_article_like
        except Exception:
            return None, None, None, False

    def _parse_iso_dt(self, s: Optional[str]) -> Optional[datetime]:
        if not s:
            return None
        try:
            s = s.replace("Z", "+00:00")
            dt = datetime.fromisoformat(s)
            if timezone.is_naive(dt):
                dt = timezone.make_aware(dt, timezone.get_current_timezone())
            return dt.astimezone(dt_timezone.utc)
        except Exception:
            return None

    # -------------------------------
    # Save + Enqueue
    # -------------------------------
    def save_article(
        self,
        title: str,
        summary: str,
        link: str,
        image_url: Optional[str],
        source_name: str,
        market: str = NewsMarket.KR,
        content: Optional[str] = None,
        published_at: Optional[datetime] = None,
    ) -> int:
        title = self._normalize_title(title)
        link_noquery = self._normalize_url_noquery(link)
        link = self._normalize_url(link)

        if not title or not link:
            return 0

        # ÏµúÏ¢Ö Î∞©Ïñ¥: Î©îÎâ¥/ÌóàÎ∏å Ï†úÎ™© Ïª∑
        if self._looks_like_menu_or_section_title(title):
            return 0

        # ÏµúÏ¢Ö Î∞©Ïñ¥: Í∏∞ÏÇ¨Ìòï URL ÏïÑÎãàÎ©¥ Ïª∑
        if not self._looks_like_article_url(link):
            return 0

        if self._is_duplicate(title, link_noquery):
            self.stdout.write(f"  - [{source_name}] (Ï§ëÎ≥µ) {title[:30]}...")
            return 0

        valid_image_url = self._pick_valid_image_url(image_url)
        if not valid_image_url:
            self.stdout.write(f"  - [{source_name}] (Ïù¥ÎØ∏ÏßÄ ÏóÜÏùå/Í≤ÄÏ¶ùÏã§Ìå®) {title[:30]}... -> keep(no image)")

        pub_utc = self._to_utc(published_at)

        try:
            vecs = embed_passages([(summary or title).strip()])
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"‚ùå ÏûÑÎ≤†Îî© Ïã§Ìå®: {e}"))
            return 0

        vector = vecs[0] if vecs else None
        if not vector:
            self.stdout.write("    -> Î°úÏª¨ ÏûÑÎ≤†Îî© Ïã§Ìå®Î°ú Ï†ÄÏû• Í±¥ÎÑàÎúÄ")
            return 0

        try:
            with transaction.atomic():
                article = NewsArticle.objects.create(
                    title=title,
                    summary=summary,
                    content=content,
                    url=link_noquery,  # Ï†ÄÏû•ÏùÄ canonical(no query)
                    image_url=valid_image_url,
                    sector=NewsSector.ETC,
                    market=market,
                    published_at=pub_utc,
                    embedding_local=vector,
                    related_name="",
                    ticker="",
                    confidence=0.0,
                )

            try:
                enqueue_article_for_classify(
                    article_id=article.id,
                    title=article.title,
                    content=(article.content or article.summary or ""),
                )
                self.stdout.write(f"  + [{source_name}] [New] {title[:30]}... (sector=ETC -> queued)")
            except Exception as e:
                self.stdout.write(self.style.WARNING(f"    -> enqueue Ïã§Ìå®(Î¨¥ÏãúÌïòÍ≥† Í≥ÑÏÜç): {e}"))

            return 1
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"    -> DB Ï†ÄÏû• Ïã§Ìå®: {e}"))
            return 0

    # ---------------------------------------------------------------------
    # Generic crawler
    # ---------------------------------------------------------------------
    def _is_candidate_link(self, href: str, source: GenericSource) -> bool:
        h = (href or "").strip()
        if not h or h in self.BAD_HREF_EXACT:
            return False

        low = h.lower()
        if any(low.startswith(p) for p in self.BAD_HREF_PREFIXES):
            return False

        for ex in source.exclude_any:
            if ex and ex in h:
                return False

        if source.must_contain_any:
            ok = False
            for m in source.must_contain_any:
                if m and m in h:
                    ok = True
                    break
            if not ok:
                return False

        if source.must_match_regex:
            ok = False
            for rpat in source.must_match_regex:
                if rpat and re.search(rpat, h):
                    ok = True
                    break
            if not ok:
                return False

        return True

    def _absolute_link(self, href: str, source: GenericSource, list_url: str) -> str:
        h = (href or "").strip()
        if not h:
            return ""

        if h.startswith("http://") or h.startswith("https://"):
            return self._normalize_url(h)

        if source.domain_prefix:
            return self._normalize_url(urljoin(source.domain_prefix, h))

        base = list_url if list_url.startswith("http") else source.base_url
        abs_url = urljoin(base, h)
        return self._normalize_url(abs_url)

    def crawl_generic_sources(self) -> int:
        total = 0
        self.stdout.write("\n=========================================")
        self.stdout.write("üß© Generic Sources crawling start")
        self.stdout.write(f"- sources: {len(self.GENERIC_SOURCES)}")
        self.stdout.write("=========================================")

        for s in self.GENERIC_SOURCES:
            self.stdout.write(f"\n>>> [GENERIC] {s.name}")
            for list_url in s.urls:
                self.stdout.write(f"  - list: {list_url}")
                count = 0
                processed: set[str] = set()

                try:
                    res = self.session.get(list_url, timeout=12)
                    if res.status_code >= 400:
                        self.stdout.write(self.style.WARNING(f"    -> list fetch fail: {res.status_code}"))
                        continue

                    soup = BeautifulSoup(res.text, "html.parser")
                    anchors = soup.find_all("a", href=True)

                    for a in anchors:
                        if count >= self.MAX_PER_SOURCE:
                            break

                        href = (a.get("href") or "").strip()
                        if not self._is_candidate_link(href, s):
                            continue

                        link = self._absolute_link(href, s, list_url)
                        if not link:
                            continue

                        if not self._looks_like_article_url(link):
                            continue

                        link_noquery = self._normalize_url_noquery(link)
                        if link_noquery in processed:
                            continue
                        processed.add(link_noquery)

                        raw_title = (a.get_text(" ", strip=True) or "").strip()
                        title = self._normalize_title(raw_title)
                        if self._looks_like_menu_or_section_title(title):
                            continue

                        og_img, og_desc, pub_dt, og_is_article = self._fetch_og(link)

                        # OG/JSON-LD Í∏∞ÏÇ¨ Îã®ÏÑú ÏóÜÏúºÎ©¥ Ïª∑(ÌóàÎ∏å Ï†úÍ±∞ Î™©Ï†Å)
                        if not og_is_article and not pub_dt:
                            continue

                        summary = (og_desc or title).strip()
                        summary = self.TITLE_MULTI_SPACE_RE.sub(" ", summary).strip()

                        inc = self.save_article(
                            title=title,
                            summary=summary,
                            link=link,
                            image_url=og_img,
                            source_name=s.name,
                            market=NewsMarket.KR,
                            published_at=pub_dt,
                        )
                        count += inc
                        total += inc
                        time.sleep(self.SLEEP_BETWEEN_ITEMS)

                except Exception as e:
                    self.stdout.write(self.style.ERROR(f"    -> generic crawl error: {e}"))

                time.sleep(self.SLEEP_BETWEEN_SECTIONS)

        return total

    # ---------------------------------------------------------------------
    # Specialized crawlers
    # ---------------------------------------------------------------------
    def crawl_naver(self) -> int:
        total = 0
        for url in self.NAVER_URLS:
            self.stdout.write(f"\n>>> [1/5] ÎÑ§Ïù¥Î≤ÑÏ¶ùÍ∂å ÌÅ¨Î°§ÎßÅ Ï§ë... url={url}")
            count = 0
            try:
                response = self.session.get(url, timeout=10)
                response.encoding = "cp949"
                soup = BeautifulSoup(response.text, "html.parser")

                candidates = soup.find_all("a", href=True)
                processed = set()

                for a in candidates:
                    if count >= self.MAX_PER_SOURCE:
                        break
                    try:
                        href = (a.get("href") or "").strip()
                        if not href:
                            continue

                        if "read.naver" not in href and "/news/" not in href:
                            continue

                        link = urljoin("https://finance.naver.com", href)
                        link = self._normalize_url(link)

                        if not self._looks_like_article_url(link):
                            continue

                        link_noquery = self._normalize_url_noquery(link)
                        if not link_noquery or link_noquery in processed:
                            continue
                        processed.add(link_noquery)

                        title = self._normalize_title((a.get_text(" ", strip=True) or "").strip())
                        if self._looks_like_menu_or_section_title(title):
                            continue

                        og_img, og_desc, pub_dt, og_is_article = self._fetch_og(link)
                        if not og_is_article and not pub_dt:
                            continue

                        summary = (og_desc or title).strip()

                        inc = self.save_article(
                            title=title,
                            summary=summary,
                            link=link,
                            image_url=og_img,
                            source_name="Naver",
                            market=NewsMarket.KR,
                            published_at=pub_dt,
                        )
                        count += inc
                        total += inc
                        time.sleep(self.SLEEP_BETWEEN_ITEMS)
                    except Exception:
                        continue

            except Exception as e:
                self.stdout.write(self.style.ERROR(f"‚ùå ÎÑ§Ïù¥Î≤ÑÏ¶ùÍ∂å ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

            time.sleep(self.SLEEP_BETWEEN_SECTIONS)

        return total

    def crawl_yonhap(self) -> int:
        total = 0
        for url in self.YONHAP_URLS:
            self.stdout.write(f"\n>>> [2/5] Ïó∞Ìï©Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ï§ë... url={url}")
            count = 0
            try:
                response = self.session.get(url, timeout=10)
                response.encoding = response.apparent_encoding
                soup = BeautifulSoup(response.text, "html.parser")

                candidates = soup.find_all("a", href=True)
                processed_links = set()

                for a_tag in candidates:
                    if count >= self.MAX_PER_SOURCE:
                        break
                    try:
                        href = (a_tag.get("href") or "").strip()
                        if not href:
                            continue

                        if "/view/" not in href:
                            continue

                        link = urljoin("https://www.yna.co.kr", href)
                        link = self._normalize_url(link)

                        if not self._looks_like_article_url(link):
                            continue

                        link_noquery = self._normalize_url_noquery(link)
                        if not link_noquery or link_noquery in processed_links:
                            continue
                        processed_links.add(link_noquery)

                        title = self._normalize_title((a_tag.get_text(" ", strip=True) or "").strip())
                        if self._looks_like_menu_or_section_title(title):
                            continue

                        if len(title) < 12:
                            continue

                        og_img, og_desc, pub_dt, og_is_article = self._fetch_og(link)
                        if not og_is_article and not pub_dt:
                            continue

                        summary = (og_desc or title).strip()

                        inc = self.save_article(
                            title=title,
                            summary=summary,
                            link=link,
                            image_url=og_img,
                            source_name="Yonhap",
                            market=NewsMarket.KR,
                            published_at=pub_dt,
                        )
                        count += inc
                        total += inc
                        time.sleep(self.SLEEP_BETWEEN_ITEMS)
                    except Exception:
                        continue

            except Exception as e:
                self.stdout.write(self.style.ERROR(f"‚ùå Ïó∞Ìï©Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

            time.sleep(self.SLEEP_BETWEEN_SECTIONS)

        return total

    def crawl_hankyung(self) -> int:
        total = 0
        for url in self.HANKYUNG_URLS:
            self.stdout.write(f"\n>>> [3/5] ÌïúÍµ≠Í≤ΩÏ†ú(Hankyung) ÌÅ¨Î°§ÎßÅ Ï§ë... url={url}")
            count = 0
            try:
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.text, "html.parser")

                candidates = soup.find_all("a", href=True)
                processed = set()

                for a in candidates:
                    if count >= self.MAX_PER_SOURCE:
                        break
                    try:
                        href = (a.get("href") or "").strip()
                        if not href:
                            continue

                        if "/article/" not in href:
                            continue

                        link = href if href.startswith("http") else urljoin("https://www.hankyung.com", href)
                        link = self._normalize_url(link)

                        if not self._looks_like_article_url(link):
                            continue

                        link_noquery = self._normalize_url_noquery(link)
                        if not link_noquery or link_noquery in processed:
                            continue
                        processed.add(link_noquery)

                        title = self._normalize_title((a.get_text(" ", strip=True) or "").strip())
                        if self._looks_like_menu_or_section_title(title):
                            continue

                        og_img, og_desc, pub_dt, og_is_article = self._fetch_og(link)
                        if not og_is_article and not pub_dt:
                            continue

                        summary = (og_desc or title).strip()

                        inc = self.save_article(
                            title=title,
                            summary=summary,
                            link=link,
                            image_url=og_img,
                            source_name="Hankyung",
                            market=NewsMarket.KR,
                            published_at=pub_dt,
                        )
                        count += inc
                        total += inc
                        time.sleep(self.SLEEP_BETWEEN_ITEMS)
                    except Exception:
                        continue

            except Exception as e:
                self.stdout.write(self.style.ERROR(f"‚ùå ÌïúÍµ≠Í≤ΩÏ†ú ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

            time.sleep(self.SLEEP_BETWEEN_SECTIONS)

        return total

    def crawl_mk(self) -> int:
        total = 0
        for url in self.MK_URLS:
            self.stdout.write(f"\n>>> [4/5] Îß§ÏùºÍ≤ΩÏ†ú(MK) ÌÅ¨Î°§ÎßÅ Ï§ë... url={url}")
            count = 0
            try:
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.text, "html.parser")

                candidates = soup.find_all("a", href=True)
                processed = set()

                for a in candidates:
                    if count >= self.MAX_PER_SOURCE:
                        break
                    try:
                        href = (a.get("href") or "").strip()
                        if not href:
                            continue

                        if "/news/" not in href:
                            continue

                        link = href if href.startswith("http") else urljoin("https://www.mk.co.kr", href)
                        link = self._normalize_url(link)

                        if not self._looks_like_article_url(link):
                            continue

                        link_noquery = self._normalize_url_noquery(link)
                        if not link_noquery or link_noquery in processed:
                            continue
                        processed.add(link_noquery)

                        title = self._normalize_title((a.get_text(" ", strip=True) or "").strip())
                        if self._looks_like_menu_or_section_title(title):
                            continue

                        og_img, og_desc, pub_dt, og_is_article = self._fetch_og(link)
                        if not og_is_article and not pub_dt:
                            continue

                        summary = (og_desc or title).strip()

                        inc = self.save_article(
                            title=title,
                            summary=summary,
                            link=link,
                            image_url=og_img,
                            source_name="MK",
                            market=NewsMarket.KR,
                            published_at=pub_dt,
                        )
                        count += inc
                        total += inc
                        time.sleep(self.SLEEP_BETWEEN_ITEMS)
                    except Exception:
                        continue

            except Exception as e:
                self.stdout.write(self.style.ERROR(f"‚ùå Îß§ÏùºÍ≤ΩÏ†ú ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

            time.sleep(self.SLEEP_BETWEEN_SECTIONS)

        return total

    def crawl_chosun(self) -> int:
        total = 0
        for url in self.CHOSUN_URLS:
            self.stdout.write(f"\n>>> [5/5] Ï°∞ÏÑ†ÏùºÎ≥¥ ÌÅ¨Î°§ÎßÅ Ï§ë... url={url}")
            count = 0
            try:
                response = self.session.get(url, timeout=10)
                response.encoding = response.apparent_encoding
                soup = BeautifulSoup(response.text, "html.parser")

                candidates = soup.find_all("a", href=True)
                processed = set()

                for a in candidates:
                    if count >= self.MAX_PER_SOURCE:
                        break
                    try:
                        href = (a.get("href") or "").strip()
                        if not href or href == "#":
                            continue

                        low = href.lower()
                        if any(low.startswith(p) for p in self.BAD_HREF_PREFIXES):
                            continue

                        link = href if href.startswith("http") else urljoin("https://www.chosun.com", href)
                        link = self._normalize_url(link)

                        if not link.startswith("https://www.chosun.com/"):
                            continue

                        if not (self.ARTICLE_DATE_RE.search(link) or self.ARTICLE_HTMLDIR_RE.search(link)):
                            continue

                        link_noquery = self._normalize_url_noquery(link)
                        if link_noquery in processed:
                            continue
                        processed.add(link_noquery)

                        title = self._normalize_title((a.get_text(" ", strip=True) or "").strip())
                        if self._looks_like_menu_or_section_title(title):
                            continue

                        og_img, og_desc, pub_dt, og_is_article = self._fetch_og(link)

                        if not og_is_article and not pub_dt:
                            continue
                        if not og_desc:
                            continue  # Ï°∞ÏÑ†ÏùÄ ÌóàÎ∏å ÏÑûÏûÑÏù¥ ÎßéÏïÑ desc ÏóÜÎäî Í±¥ Ï†úÏô∏

                        summary = (og_desc or title).strip()

                        inc = self.save_article(
                            title=title,
                            summary=summary,
                            link=link,
                            image_url=og_img,
                            source_name="Chosun",
                            market=NewsMarket.KR,
                            published_at=pub_dt,
                        )
                        count += inc
                        total += inc
                        time.sleep(self.SLEEP_BETWEEN_ITEMS)
                    except Exception:
                        continue

            except Exception as e:
                self.stdout.write(self.style.ERROR(f"‚ùå Ï°∞ÏÑ†ÏùºÎ≥¥ ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

            time.sleep(self.SLEEP_BETWEEN_SECTIONS)

        return total
