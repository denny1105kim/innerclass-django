# news/management/commands/crawl_news.py
from __future__ import annotations

import re
import time
from datetime import datetime, timezone as dt_timezone
from typing import Optional
from urllib.parse import urljoin, urlparse, urlsplit, urlunsplit

import requests
from bs4 import BeautifulSoup
from django.conf import settings
from django.core.management.base import BaseCommand
from django.db import transaction
from django.utils import timezone

import openai

from news.models import NewsArticle


class Command(BaseCommand):
    """
    Íµ≠ÎÇ¥ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ (ÏÑπÏÖò URL Í∏∞Î∞ò Í≥µÌÜµ Î°úÏßÅ Ï†ÅÏö©)
    - ÎßÅÌÅ¨ ÌõÑÎ≥¥ ÏàòÏßë -> Î©îÎâ¥/ÏÑπÏÖò/ÌóàÎ∏å Ï†úÍ±∞ -> Í∏∞ÏÇ¨ Í∞ÄÎä•ÏÑ± ÌåêÎ≥Ñ -> ÎîîÌÖåÏùº(OG/JSON-LD)Î°ú ÌôïÏ†ï
    - Ï†ÄÏû•(OpenAI embedding) + analyze_news(save_to_db=True)Î°ú theme/Lv1~Lv5 Ï†ÄÏû•
    """

    help = "Íµ≠ÎÇ¥(ÎÑ§Ïù¥Î≤ÑÍ∏àÏúµ/Ïó∞Ìï©Ïù∏Ìè¨Îß•Ïä§/ÌïúÍµ≠Í≤ΩÏ†ú/Îß§ÏùºÍ≤ΩÏ†ú) Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ÌõÑ DB Ï†ÄÏû•(+theme/Lv1~Lv5 ÏÑ†Ìñâ Î∂ÑÏÑù)."

    # -------------------------
    # Crawling limits / pacing
    # -------------------------
    MAX_PER_SOURCE = 80
    SLEEP_BETWEEN_ITEMS = 0.08
    SLEEP_BETWEEN_SOURCES = 0.25

    USER_AGENT = (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
    )

    # -------------------------
    # Image filtering (ÏôÑÌôî)
    # -------------------------
    BAD_IMAGE_PATTERNS = [
        r"placeholder",
        r"default",
        r"no[-_ ]?image",
        r"no[-_ ]?photo",
        r"image[-_ ]?not[-_ ]?available",
        r"not[-_ ]?found",
        r"spacer",
        r"sprite",
        r"blank",
        r"transparent",
        r"1x1",
        r"pixel",
        r"favicon",
    ]
    BAD_PATH_EXT = (".html", ".htm", ".php", ".aspx", ".jsp")

    VALIDATE_IMAGE_HEAD = True
    IMAGE_HEAD_TIMEOUT = 4
    MAX_IMAGE_BYTES = 10 * 1024 * 1024  # 10MB

    # -------------------------
    # URL/Title filtering Í∞ïÌôî
    # -------------------------
    ARTICLE_DATE_RE = re.compile(r"/20\d{2}/\d{2}/\d{2}/")
    ARTICLE_HTMLDIR_RE = re.compile(r"/site/data/html_dir/")

    ARTICLE_LIKELY_RE_LIST = [
        ARTICLE_DATE_RE,
        ARTICLE_HTMLDIR_RE,
        re.compile(r"/article/"),
        re.compile(r"/news/view"),
        re.compile(r"/news/read"),
        re.compile(r"/news/articleView\.html"),
        re.compile(r"/view\.php"),
        re.compile(r"/view/"),
        re.compile(r"/mtview\.php"),
        re.compile(r"/NewsView/"),
        re.compile(r"/news/view/"),
        re.compile(r"/news/article/"),
    ]

    NON_ARTICLE_URL_RE_LIST = [
        re.compile(r"/(search|login|member|subscription|subscribe|mypage)(/|$)"),
        re.compile(r"/(photo|video|vod|podcast|gallery)(/|$)"),
        re.compile(r"/(section|category|categories|tag|tags|topic|topics)(/|$)"),
        re.compile(r"/(company|about|notice|event|press|policy)(/|$)"),
        re.compile(r"/news/?$"),
        re.compile(r"/news/section"),
        re.compile(r"/NewsList/"),
        re.compile(r"/Stock/?$"),
        re.compile(r"/economy/?$"),
        re.compile(r"/industry/?$"),
        re.compile(r"/stock/?$"),
        re.compile(r"/it/?$"),
        re.compile(r"/weeklybiz/?$"),
        re.compile(r"/(lists|list)\b"),
    ]

    MENU_TITLE_KEYWORDS = (
        "Î∞îÎ°úÍ∞ÄÍ∏∞",
        "Í≥µÏßÄ",
        "ÏïåÎ¶º",
        "ÎçîÎ≥¥Í∏∞",
        "Ï†ÑÏ≤¥Î≥¥Í∏∞",
        "Ï†ÑÏ≤¥",
        "Í≤ÄÏÉâ",
        "Î°úÍ∑∏Ïù∏",
        "Íµ¨ÎèÖ",
        "Î©§Î≤ÑÏã≠",
        "ÌöåÏõê",
        "Î©îÎâ¥",
        "ÏÑπÏÖò",
        "Ïπ¥ÌÖåÍ≥†Î¶¨",
        "ÎùºÏù¥Î∏å",
        "ÏòÅÏÉÅ",
        "Ìè¨ÌÜ†",
        "ÏÇ¨ÏßÑ",
        "Í∞§Îü¨Î¶¨",
        "Í∏∞Ìöç",
        "ÏπºÎüº",
        "ÏÇ¨ÏÑ§",
        "Ïò§ÌîºÎãàÏñ∏",
        "Í∏∞ÏûêÏùò",
        "ÌäπÌååÏõê",
        "Ï†ÑÎ¨∏Í∞Ä",
        "ÏãúÍ∞Å",
        "Î∞©ÏÜ°",
        "ÎØ∏ÎîîÏñ¥",
        "IT¬∑Ïù∏ÌÑ∞ÎÑ∑",
        "Ï†ÑÍ∏∞¬∑Ï†ÑÏûê¬∑ÌÜµÏã†",
        "ÊúùÈÆÆÏπºÎüº",
        "The Column",
        "Desk pick",
        "special edition",
        "Ïä§ÌéòÏÖúÏóêÎîîÏÖò",
    )
    MENU_TITLE_SHORT_RE = re.compile(r"^(Íµ≠ÎÇ¥|Ìï¥Ïô∏|Í≤ΩÏ†ú|ÏÇ∞ÏóÖ|Ï¶ùÍ∂å|Ï†ïÏπò|ÏÇ¨Ìöå|Íµ≠Ï†ú|Î¨∏Ìôî|Ïä§Ìè¨Ï∏†|Ïó∞Ïòà|IT|ÌÖåÌÅ¨)$")

    BAD_HREF_PREFIXES = ("javascript:", "mailto:", "tel:")
    BAD_HREF_EXACT = ("#", "")

    TITLE_DATE_TIME_RE = re.compile(r"(20\d{2}[-./]\d{2}[-./]\d{2})(\s+\d{2}:\d{2})?")
    TITLE_ONLY_PIPES_RE = re.compile(r"^[\s\|\-‚Äì‚Äî¬∑‚Ä¢\u00b7]+$")
    TITLE_ARROW_RE = re.compile(r"[‚ùØ‚Ä∫¬ª>]+")
    TITLE_MULTI_SPACE_RE = re.compile(r"\s+")

    # -------------------------
    # Source URLs (Í∏∞Ï°¥ crawler Í∏∞Î∞ò)
    # -------------------------
    NAVER_LIST_URL = "https://finance.naver.com/news/mainnews.naver"
    YONHAP_LIST_URL = "https://news.einfomax.co.kr/news/articleList.html?sc_section_code=S1N1"
    HANKYUNG_LIST_URL = "https://www.hankyung.com/economy"
    MK_LIST_URL = "https://www.mk.co.kr/news/economy/"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": self.USER_AGENT})

    # -------------------------------
    # OpenAI embedding
    # -------------------------------
    def get_embedding(self, text: str):
        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        resp = client.embeddings.create(input=text, model="text-embedding-3-small")
        return resp.data[0].embedding

    # -------------------------------
    # URL helpers
    # -------------------------------
    def _normalize_url(self, url: str) -> str:
        u = (url or "").strip()
        if not u:
            return ""
        try:
            parts = urlsplit(u)
            # query Ïú†ÏßÄ(Ïñ∏Î°†ÏÇ¨Î≥Ñ view ÌååÎùºÎØ∏ÌÑ∞Í∞Ä Í∏∞ÏÇ¨ ÏãùÎ≥ÑÏóê Ïì∞Ïù¥Îäî Í≤ΩÏö∞Í∞Ä ÏûàÏñ¥ Ïú†ÏßÄ)
            return urlunsplit((parts.scheme, parts.netloc, parts.path, parts.query, ""))
        except Exception:
            return u

    def _normalize_url_noquery(self, url: str) -> str:
        u = (url or "").strip()
        if not u:
            return ""
        try:
            parts = urlsplit(u)
            return urlunsplit((parts.scheme, parts.netloc, parts.path, "", ""))
        except Exception:
            return u

    # -------------------------------
    # Title helpers
    # -------------------------------
    def _clean_title_text(self, raw: str) -> str:
        t = (raw or "").strip()
        if not t:
            return ""
        t = t.replace("\n", " ").replace("\r", " ").replace("\t", " ")
        t = self.TITLE_MULTI_SPACE_RE.sub(" ", t).strip()
        t = self.TITLE_ARROW_RE.sub("", t).strip()
        t = self.TITLE_DATE_TIME_RE.sub("", t).strip()
        t = t.strip(" |¬∑‚Ä¢-‚Äì‚Äî>‚Ä∫¬ª‚ùØ")
        t = self.TITLE_MULTI_SPACE_RE.sub(" ", t).strip()
        return t[:500]

    def _normalize_title(self, title: str) -> str:
        t = title or ""
        t = re.sub(r"^[\d\.\s]+", "", t)
        t = self._clean_title_text(t)
        return t[:500]

    # -------------------------------
    # duplicate
    # -------------------------------
    def _is_duplicate(self, title: str, url: str) -> bool:
        title_n = self._normalize_title(title)
        url_n = self._normalize_url_noquery(url)

        if title_n and NewsArticle.objects.filter(title=title_n).exists():
            return True
        if url_n and NewsArticle.objects.filter(url=url_n).exists():
            return True
        return False

    # -------------------------------
    # menu/section detection
    # -------------------------------
    def _looks_like_menu_or_section_title(self, title: str) -> bool:
        t = (title or "").strip()
        if not t:
            return True
        if self.MENU_TITLE_SHORT_RE.match(t):
            return True
        if len(t) < 8:
            return True
        if self.TITLE_ONLY_PIPES_RE.match(t):
            return True

        low = t.lower()
        for kw in self.MENU_TITLE_KEYWORDS:
            if kw and kw.lower() in low:
                return True

        if "¬∑" in t and len(t) <= 16:
            return True
        return False

    def _looks_like_article_url(self, url: str) -> bool:
        u = (url or "").strip()
        if not u:
            return False

        for rx in self.NON_ARTICLE_URL_RE_LIST:
            if rx.search(u):
                return False

        if self.ARTICLE_DATE_RE.search(u) or self.ARTICLE_HTMLDIR_RE.search(u):
            return True

        for rx in self.ARTICLE_LIKELY_RE_LIST:
            if rx.search(u):
                return True

        return False

    # -------------------------------
    # Image validation
    # -------------------------------
    def _looks_like_bad_image_url(self, image_url: str) -> bool:
        u = (image_url or "").strip()
        if not u:
            return True
        if not (u.startswith("http://") or u.startswith("https://")):
            return True
        path = urlparse(u).path.lower()
        if path.endswith(self.BAD_PATH_EXT):
            return True
        low = u.lower()
        for pat in self.BAD_IMAGE_PATTERNS:
            if re.search(pat, low):
                return True
        return False

    def _is_real_image_by_head(self, image_url: str) -> bool:
        try:
            r = self.session.head(image_url, timeout=self.IMAGE_HEAD_TIMEOUT, allow_redirects=True)
            if r.status_code >= 400:
                return False

            ctype = (r.headers.get("Content-Type") or "").lower()
            clen = r.headers.get("Content-Length")
            if clen:
                try:
                    if int(clen) > self.MAX_IMAGE_BYTES:
                        return False
                except Exception:
                    pass

            if ctype.startswith("image/"):
                return True

            rg = self.session.get(
                image_url,
                timeout=self.IMAGE_HEAD_TIMEOUT,
                allow_redirects=True,
                stream=True,
                headers={"Range": "bytes=0-2047"},
            )
            if rg.status_code >= 400:
                return False

            ctype2 = (rg.headers.get("Content-Type") or "").lower()
            return ctype2.startswith("image/")
        except Exception:
            return False

    def _pick_valid_image_url(self, image_url: Optional[str]) -> Optional[str]:
        u = (image_url or "").strip()
        if not u:
            return None
        if self._looks_like_bad_image_url(u):
            return None
        if self.VALIDATE_IMAGE_HEAD and not self._is_real_image_by_head(u):
            return None
        return u

    # -------------------------------
    # Time helpers (UTC normalize)
    # -------------------------------
    def _to_utc(self, dt: Optional[datetime]) -> datetime:
        if not dt:
            now = timezone.now()
            if timezone.is_naive(now):
                now = timezone.make_aware(now, timezone.get_current_timezone())
            return now.astimezone(dt_timezone.utc)

        if timezone.is_naive(dt):
            dt = timezone.make_aware(dt, timezone.get_current_timezone())

        return dt.astimezone(dt_timezone.utc)

    def _parse_iso_dt(self, s: Optional[str]) -> Optional[datetime]:
        if not s:
            return None
        try:
            s = s.replace("Z", "+00:00")
            dt = datetime.fromisoformat(s)
            if timezone.is_naive(dt):
                dt = timezone.make_aware(dt, timezone.get_current_timezone())
            return dt.astimezone(dt_timezone.utc)
        except Exception:
            return None

    # -------------------------------
    # Detail fetch (OG + JSON-LD)
    # -------------------------------
    def _fetch_detail_signals(
        self, url: str
    ) -> tuple[Optional[str], Optional[str], Optional[datetime], Optional[str], bool]:
        """
        return: (og_image, og_desc, published_at, content_text, is_article_like)
        """
        try:
            res = self.session.get(url, timeout=10)
            if res.status_code >= 400:
                return None, None, None, None, False

            soup = BeautifulSoup(res.text, "html.parser")

            og_image = None
            og_desc = None
            published_at = None

            m_img = soup.find("meta", property="og:image")
            if m_img and m_img.get("content"):
                og_image = (m_img.get("content") or "").strip()

            m_desc = soup.find("meta", property="og:description")
            if m_desc and m_desc.get("content"):
                og_desc = (m_desc.get("content") or "").strip()

            m_pub = soup.find("meta", property="article:published_time")
            if m_pub and m_pub.get("content"):
                published_at = self._parse_iso_dt(m_pub.get("content"))

            # Í∏∞ÏÇ¨ Îã®ÏÑú: og:type/article ÎòêÎäî JSON-LD NewsArticle
            is_article_like = False
            og_type = soup.find("meta", property="og:type")
            if og_type and (og_type.get("content") or "").strip().lower() in ("article", "news", "newsarticle"):
                is_article_like = True

            if not is_article_like:
                for s in soup.find_all("script", attrs={"type": "application/ld+json"})[:10]:
                    txt = (s.get_text() or "").strip()
                    if not txt:
                        continue
                    low = txt.lower()
                    if '"@type"' in low and ("newsarticle" in low or '"article"' in low or '"reportage"' in low):
                        is_article_like = True
                        break

            # Î≥∏Î¨∏(Í∞ÄÎ≤ºÏö¥ Ï†ÄÏû•Ïö©): ÎÑàÎ¨¥ Í∏∏Î©¥ ÏûêÎ•¥Í∏∞
            content_text = None
            # ÏÇ¨Ïù¥Ìä∏Î≥Ñ Í≥µÌÜµ selectorÎäî Ïñ¥Î†§ÏõåÏÑú, Í∞ÄÏû• ÏïàÏ†ÑÌïú "article" Ïö∞ÏÑ†
            article_tag = soup.find("article")
            if article_tag:
                content_text = article_tag.get_text("\n", strip=True)
            else:
                # infomax selector fallback
                div = soup.select_one("#article-view-content-div")
                if div:
                    content_text = div.get_text("\n", strip=True)

            if content_text:
                content_text = content_text.strip()
                content_text = content_text[:4000] if len(content_text) > 4000 else content_text

            return og_image, og_desc, published_at, content_text, is_article_like
        except Exception:
            return None, None, None, None, False

    # -------------------------------
    # Save + Analyze (theme/Lv1~Lv5)
    # -------------------------------
    def save_article(
        self,
        *,
        title: str,
        summary: str,
        link: str,
        image_url: Optional[str],
        source_name: str,
        sector: str = "Í∏àÏúµ/Í≤ΩÏ†ú",
        market: str = "Korea",
        content: Optional[str] = None,
        published_at: Optional[datetime] = None,
    ) -> int:
        title = self._normalize_title(title)
        link = self._normalize_url(link)
        link_noquery = self._normalize_url_noquery(link)

        if not title or not link:
            return 0

        if self._looks_like_menu_or_section_title(title):
            return 0
        if not self._looks_like_article_url(link):
            return 0
        if self._is_duplicate(title, link_noquery):
            self.stdout.write(f"  - [{source_name}] (Ï§ëÎ≥µ) {title[:30]}...")
            return 0

        # embedding
        emb_text = (summary or "").strip() or title
        try:
            vector = self.get_embedding(emb_text)
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"‚ö†Ô∏è ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå®: {e}"))
            return 0

        try:
            with transaction.atomic():
                article = NewsArticle.objects.create(
                    title=title,
                    summary=summary,
                    content=content,
                    url=link_noquery,  # canonical(no query)
                    image_url=image_url,
                    sector=sector,
                    market=market,
                    published_at=published_at or timezone.now(),
                    embedding=vector,
                )

                # ‚úÖ theme + Lv1~Lv5 Ï†ÄÏû•
                from news.services.analyze_news import analyze_news
                analyze_news(article, save_to_db=True)

            self.stdout.write(f"  + [{source_name}] [New] {title[:40]}... (analyzed)")
            return 1
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"    -> DB Ï†ÄÏû• Ïã§Ìå®: {e}"))
            return 0

    # =========================================================================
    # Command entry
    # =========================================================================
    def handle(self, *args, **kwargs):
        if not getattr(settings, "OPENAI_API_KEY", None):
            self.stdout.write(self.style.ERROR("settings.OPENAI_API_KEY Í∞Ä ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏßÄ ÏïäÏäµÎãàÎã§."))
            return

        self.stdout.write("=========================================")
        self.stdout.write("üì° Íµ≠ÎÇ¥ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ (ÏÑπÏÖò/Î©îÎâ¥ Ï†úÍ±∞ + OG/JSON-LD Í∏∞ÏÇ¨ ÌåêÎ≥Ñ)")
        self.stdout.write("=========================================")

        total_saved = 0
        total_saved += self.crawl_naver()
        time.sleep(self.SLEEP_BETWEEN_SOURCES)

        total_saved += self.crawl_yonhap_infomax()
        time.sleep(self.SLEEP_BETWEEN_SOURCES)

        total_saved += self.crawl_hankyung()
        time.sleep(self.SLEEP_BETWEEN_SOURCES)

        total_saved += self.crawl_mk()

        self.stdout.write("=========================================")
        self.stdout.write(self.style.SUCCESS(f"‚úÖ ÌÜµÌï© ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å. (Ï¥ù Ïã†Í∑ú Ï†ÄÏû•: {total_saved}Í∞ú)"))
        self.stdout.write("=========================================")

    # =========================================================================
    # 1) Naver Finance (list Íµ¨Ï°∞Í∞Ä ÏïàÏ†ïÏ†ÅÏù¥Îùº list selector ÌôúÏö©)
    # =========================================================================
    def crawl_naver(self) -> int:
        self.stdout.write("\n>>> [1/4] ÎÑ§Ïù¥Î≤Ñ Í∏àÏúµ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ï§ë...")
        url = self.NAVER_LIST_URL
        headers = {"User-Agent": self.USER_AGENT}

        saved = 0
        try:
            res = self.session.get(url, headers=headers, timeout=10)
            res.encoding = "cp949"
            soup = BeautifulSoup(res.text, "html.parser")

            items = soup.select(".mainNewsList li")
            for li in items:
                if saved >= self.MAX_PER_SOURCE:
                    break
                try:
                    a = li.select_one(".articleSubject a")
                    s = li.select_one(".articleSummary")
                    if not a or not s:
                        continue

                    title = a.get_text(strip=True)
                    link = urljoin("https://finance.naver.com", a.get("href") or "")

                    # Ïç∏ÎÑ§Ïùº(ÎÑ§Ïù¥Î≤ÑÎäî listÏóêÏÑú ÏïàÏ†ïÏ†Å)
                    image_url = None
                    img = li.select_one("img")
                    if img and img.get("src"):
                        base = (img.get("src") or "").split("?")[0]
                        image_url = f"{base}?type=w660"

                    raw_summary = s.get_text("\n", strip=True)
                    summary = raw_summary.split("\n")[0].strip() if raw_summary else title

                    # ÎÑ§Ïù¥Î≤ÑÎèÑ ÎîîÌÖåÏùº ÌôïÏù∏(ÌóàÎ∏å/Î©îÎâ¥ ÏÑûÏûÑ Î∞©ÏßÄ)
                    og_img, og_desc, pub_dt, content_text, is_article_like = self._fetch_detail_signals(link)
                    if not is_article_like and not pub_dt:
                        continue

                    if og_desc:
                        summary = og_desc.strip()

                    # imageÎäî og Ïö∞ÏÑ†
                    image_url = self._pick_valid_image_url(og_img or image_url)

                    inc = self.save_article(
                        title=title,
                        summary=summary,
                        link=link,
                        image_url=image_url,
                        source_name="Naver",
                        sector="Í∏àÏúµ/Í≤ΩÏ†ú",
                        market="Korea",
                        content=content_text,
                        published_at=pub_dt or timezone.now(),
                    )
                    saved += inc
                    time.sleep(self.SLEEP_BETWEEN_ITEMS)
                except Exception:
                    continue

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"‚ùå ÎÑ§Ïù¥Î≤Ñ ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

        return saved

    # =========================================================================
    # 2) Yonhap Infomax (Í∏∞ÏÇ¨ ÎßÅÌÅ¨ Ìå®ÌÑ¥Ïù¥ Î™ÖÌôï)
    # =========================================================================
    def crawl_yonhap_infomax(self) -> int:
        self.stdout.write("\n>>> [2/4] Ïó∞Ìï©Ïù∏Ìè¨Îß•Ïä§ ÌÅ¨Î°§ÎßÅ Ï§ë...")
        url = self.YONHAP_LIST_URL
        headers = {"User-Agent": self.USER_AGENT}

        saved = 0
        try:
            res = self.session.get(url, headers=headers, timeout=10)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, "html.parser")

            anchors = soup.find_all("a", href=True)
            processed = set()

            for a in anchors:
                if saved >= self.MAX_PER_SOURCE:
                    break
                try:
                    href = (a.get("href") or "").strip()
                    if not href or href in self.BAD_HREF_EXACT:
                        continue
                    if any(href.lower().startswith(p) for p in self.BAD_HREF_PREFIXES):
                        continue

                    if "articleView.html" not in href or "idxno" not in href:
                        continue

                    link = href if href.startswith("http") else urljoin("https://news.einfomax.co.kr", href)
                    link = self._normalize_url(link)
                    link_noquery = self._normalize_url_noquery(link)

                    if link_noquery in processed:
                        continue
                    processed.add(link_noquery)

                    title = self._normalize_title(a.get_text(" ", strip=True) or "")
                    if self._looks_like_menu_or_section_title(title):
                        continue
                    if len(title) < 12:
                        continue

                    og_img, og_desc, pub_dt, content_text, is_article_like = self._fetch_detail_signals(link)
                    if not is_article_like and not pub_dt:
                        continue

                    summary = (og_desc or title).strip()
                    image_url = self._pick_valid_image_url(og_img)

                    inc = self.save_article(
                        title=title,
                        summary=summary,
                        link=link,
                        image_url=image_url,
                        source_name="Infomax",
                        sector="Í∏àÏúµ/Í≤ΩÏ†ú",
                        market="Korea",
                        content=content_text,
                        published_at=pub_dt or timezone.now(),
                    )
                    saved += inc
                    time.sleep(self.SLEEP_BETWEEN_ITEMS)
                except Exception:
                    continue

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"‚ùå Ïó∞Ìï©Ïù∏Ìè¨Îß•Ïä§ ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

        return saved

    # =========================================================================
    # 3) Hankyung (listÏóêÏÑú anchor ÎåÄÎüâ -> ÎîîÌÖåÏùºÎ°ú ÌôïÏ†ï)
    # =========================================================================
    def crawl_hankyung(self) -> int:
        self.stdout.write("\n>>> [3/4] ÌïúÍµ≠Í≤ΩÏ†ú(Hankyung) ÌÅ¨Î°§ÎßÅ Ï§ë...")
        url = self.HANKYUNG_LIST_URL
        headers = {"User-Agent": self.USER_AGENT}

        saved = 0
        try:
            res = self.session.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")

            anchors = soup.find_all("a", href=True)
            processed = set()

            for a in anchors:
                if saved >= self.MAX_PER_SOURCE:
                    break
                try:
                    href = (a.get("href") or "").strip()
                    if not href or href in self.BAD_HREF_EXACT:
                        continue
                    if any(href.lower().startswith(p) for p in self.BAD_HREF_PREFIXES):
                        continue

                    # HankyungÎäî /article/ ÎßÅÌÅ¨ ÏúÑÏ£º
                    if "/article/" not in href:
                        continue

                    link = href if href.startswith("http") else urljoin("https://www.hankyung.com", href)
                    link = self._normalize_url(link)
                    link_noquery = self._normalize_url_noquery(link)

                    if link_noquery in processed:
                        continue
                    processed.add(link_noquery)

                    title = self._normalize_title(a.get_text(" ", strip=True) or "")
                    if self._looks_like_menu_or_section_title(title):
                        continue
                    if len(title) < 12:
                        continue

                    og_img, og_desc, pub_dt, content_text, is_article_like = self._fetch_detail_signals(link)
                    if not is_article_like and not pub_dt:
                        continue

                    summary = (og_desc or title).strip()
                    image_url = self._pick_valid_image_url(og_img)

                    inc = self.save_article(
                        title=title,
                        summary=summary,
                        link=link,
                        image_url=image_url,
                        source_name="Hankyung",
                        sector="Í∏àÏúµ/Í≤ΩÏ†ú",
                        market="Korea",
                        content=content_text,
                        published_at=pub_dt or timezone.now(),
                    )
                    saved += inc
                    time.sleep(self.SLEEP_BETWEEN_ITEMS)
                except Exception:
                    continue

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"‚ùå ÌïúÍµ≠Í≤ΩÏ†ú ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

        return saved

    # =========================================================================
    # 4) MK (listÏóêÏÑú /news/ ÏúÑÏ£º + ÎîîÌÖåÏùºÎ°ú ÌôïÏ†ï)
    # =========================================================================
    def crawl_mk(self) -> int:
        self.stdout.write("\n>>> [4/4] Îß§ÏùºÍ≤ΩÏ†ú(MK) ÌÅ¨Î°§ÎßÅ Ï§ë...")
        url = self.MK_LIST_URL
        headers = {"User-Agent": self.USER_AGENT}

        saved = 0
        try:
            res = self.session.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")

            anchors = soup.find_all("a", href=True)
            processed = set()

            for a in anchors:
                if saved >= self.MAX_PER_SOURCE:
                    break
                try:
                    href = (a.get("href") or "").strip()
                    if not href or href in self.BAD_HREF_EXACT:
                        continue
                    if any(href.lower().startswith(p) for p in self.BAD_HREF_PREFIXES):
                        continue

                    # MKÎäî /news/ ÌòïÌÉúÍ∞Ä ÎßéÏùå
                    if "/news/" not in href:
                        continue

                    link = href if href.startswith("http") else urljoin("https://www.mk.co.kr", href)
                    link = self._normalize_url(link)
                    link_noquery = self._normalize_url_noquery(link)

                    if link_noquery in processed:
                        continue
                    processed.add(link_noquery)

                    title = self._normalize_title(a.get_text(" ", strip=True) or "")
                    if self._looks_like_menu_or_section_title(title):
                        continue
                    if len(title) < 12:
                        continue

                    og_img, og_desc, pub_dt, content_text, is_article_like = self._fetch_detail_signals(link)
                    if not is_article_like and not pub_dt:
                        continue

                    summary = (og_desc or title).strip()
                    image_url = self._pick_valid_image_url(og_img)

                    inc = self.save_article(
                        title=title,
                        summary=summary,
                        link=link,
                        image_url=image_url,
                        source_name="MK",
                        sector="Í∏àÏúµ/Í≤ΩÏ†ú",
                        market="Korea",
                        content=content_text,
                        published_at=pub_dt or timezone.now(),
                    )
                    saved += inc
                    time.sleep(self.SLEEP_BETWEEN_ITEMS)
                except Exception:
                    continue

        except Exception as e:
            self.stdout.write(self.style.ERROR(f"‚ùå Îß§ÏùºÍ≤ΩÏ†ú ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}"))

        return saved
