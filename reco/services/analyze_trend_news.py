from __future__ import annotations

import json
from typing import Any, Dict, Optional

from django.conf import settings
from django.db import transaction
from django.utils import timezone

from main.services.gemini_client import get_gemini_client, ChatMessage
from ..models import TrendKeywordNews, TrendKeywordNewsAnalysis


# =========================================================
# Config
# =========================================================
MODEL_NAME = getattr(settings, "GEMINI_TREND_ANALYSIS_MODEL", "")  # gemini_client ë‚´ë¶€ì—ì„œ ëª¨ë¸ ì„ íƒí•˜ë©´ ë¹„ì›Œë„ ë¨
TEMPERATURE = float(getattr(settings, "GEMINI_TREND_ANALYSIS_TEMPERATURE", 0.7))
MAX_OUTPUT_TOKENS = int(getattr(settings, "GEMINI_TREND_ANALYSIS_MAX_TOKENS", 2500))

# Lv ë¶„ì„ ì‹œ, ë„ˆë¬´ ê¸´ ë³¸ë¬¸ì€ ë¹„ìš©/ì‹œê°„ì´ ì»¤ì ¸ì„œ ì˜ë¼ì„œ ë³´ëƒ„(ì›í•˜ë©´ ëŠ˜ë¦¬ë©´ ë¨)
MAX_INPUT_CHARS = int(getattr(settings, "GEMINI_TREND_ANALYSIS_MAX_INPUT_CHARS", 6000))


# =========================================================
# JSON helpers
# =========================================================
def _strip_code_fences(text: str) -> str:
    t = (text or "").strip()
    if not t.startswith("```"):
        return t
    parts = t.split("```")
    if len(parts) < 3:
        return t
    inner = parts[1].strip()
    if inner.lower().startswith("json"):
        inner = inner[4:].strip()
    return inner.strip()


def _safe_json_load(s: str) -> Optional[Dict[str, Any]]:
    s = (s or "").strip()
    if not s:
        return None

    s = _strip_code_fences(s)

    l = s.find("{")
    r = s.rfind("}")
    if l >= 0 and r >= 0 and r > l:
        s = s[l : r + 1]

    try:
        obj = json.loads(s)
        return obj if isinstance(obj, dict) else None
    except Exception:
        return None


def _build_level_payload(full: Dict[str, Any], level_key: str) -> Dict[str, Any]:
    """
    news/services/analyze_news.py íŒ¨í„´ê³¼ ë™ì¼í•˜ë˜ theme ì œì™¸.
    ê³µí†µ(meta) + level_content[level_key] merge
    """
    common = {
        "deep_analysis_reasoning": full.get("deep_analysis_reasoning", ""),
        "keywords": full.get("keywords", []),
        "sentiment_score": full.get("sentiment_score", None),
        "vocabulary": full.get("vocabulary", []),
    }
    level_content = (full.get("level_content") or {}).get(level_key) or {}
    if not isinstance(level_content, dict):
        level_content = {}

    merged = dict(common)
    merged.update(level_content)
    return merged


def _normalize_full(full: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìµœì†Œ êµ¬ì¡° ë³´ì • (í”„ë¡ íŠ¸/DB ì•ˆì •ì„±)
    """
    if not isinstance(full.get("deep_analysis_reasoning"), str):
        full["deep_analysis_reasoning"] = ""

    if not isinstance(full.get("keywords"), list):
        full["keywords"] = []

    ss = full.get("sentiment_score", None)
    if ss is not None:
        try:
            ss_int = int(ss)
            ss_int = max(0, min(100, ss_int))
            full["sentiment_score"] = ss_int
        except Exception:
            full["sentiment_score"] = None

    if not isinstance(full.get("vocabulary"), list):
        full["vocabulary"] = []

    if not isinstance(full.get("level_content"), dict):
        full["level_content"] = {}

    # ëˆ„ë½ëœ ë ˆë²¨ í‚¤ ë³´ì •
    for k in ["lv1", "lv2", "lv3", "lv4", "lv5"]:
        if k not in full["level_content"] or not isinstance(full["level_content"].get(k), dict):
            full["level_content"][k] = {}

    return full


# =========================================================
# Prompt (Gemini)
# =========================================================
def _build_prompt(title: str, content: str) -> str:
    t = (title or "").strip()
    c = (content or "").strip()
    c = c[:MAX_INPUT_CHARS]

    # âœ… ë³€ê²½ í¬ì¸íŠ¸:
    # - lv1~lv5 summaryëŠ” ë°˜ë“œì‹œ ê¹”ë”í•œ 1ì¤„
    # - ì´ëª¨í‹°ì½˜/ì´ëª¨ì§€/íŠ¹ìˆ˜ê¸°í˜¸/ë¶ˆë¦¿/ë²ˆí˜¸/ì¤„ë°”ê¿ˆ ê¸ˆì§€ ê°•ì œ
    return f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•´ ì£¼ì„¸ìš”.
ë‹¤ë¥¸ ë§ì€ ë§ë¶™ì´ì§€ ë§ê³  ë°˜ë“œì‹œ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. (ë§ˆí¬ë‹¤ìš´/ì½”ë“œë¸”ë¡ ê¸ˆì§€)

[ì¤‘ìš”: ë§íˆ¬ ì§€ì¹¨]
- ëª¨ë“  í•„ë“œëŠ” ë°˜ë“œì‹œ 'ì¡´ëŒ“ë§'ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
  (ì˜ˆ: "~ì…ë‹ˆë‹¤/~í•©ë‹ˆë‹¤/~í•˜ì„¸ìš”/~ë©ë‹ˆë‹¤" í˜•íƒœ)
- deep_analysis_reasoning í¬í•¨, lv1~lv5ì˜ summary/bullet_points/what_is_this/why_important/stock_impact/strategy_guide/action_guide ë“±
  JSON ë‚´ ëª¨ë“  ë¬¸ìì—´ì— ì ìš©í•´ ì£¼ì„¸ìš”.
- ë°˜ë§/êµ¬ì–´ì²´("~í•´", "~í•¨", "~ì„", "~í•œë‹¤") ê¸ˆì§€ì…ë‹ˆë‹¤.

[ì¤‘ìš”: summary ì¶œë ¥ ê·œì¹™]
- lv1~lv5ì˜ summaryëŠ” ë°˜ë“œì‹œ í•œ ë¬¸ì¥(1ì¤„)ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
- ì¤„ë°”ê¿ˆ(\\n) ê¸ˆì§€ì´ë©°, summary ë¬¸ìì—´ ì•ˆì— ê°œí–‰ì´ í¬í•¨ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- summaryì— ì´ëª¨ì§€/ì´ëª¨í‹°ì½˜/ì¥ì‹ë¬¸ì(ì˜ˆ: âœ…, ğŸ”¥, ğŸ“Œ ë“±) ê¸ˆì§€ì…ë‹ˆë‹¤.
- summaryì— íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ëª©ë¡/ë¶ˆë¦¿/ë²ˆí˜¸(ì˜ˆ: "-", "â€¢", "*", "1)", "1.", "(1)") ê¸ˆì§€ì…ë‹ˆë‹¤.
- summaryëŠ” ë¬¸ì¥ë¶€í˜¸(ë§ˆì¹¨í‘œ/ì‰¼í‘œ)ëŠ” í—ˆìš©í•˜ë˜, ê³¼ë„í•œ ì¥ì‹ì€ í”¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

[ê¸°ì‚¬ ì •ë³´]
ì œëª©: {t}
ë‚´ìš©: {c}

[ì‘ë‹µ í˜•ì‹ (JSON)]
{{
  "deep_analysis_reasoning": "ì—¬ê¸°ì—ëŠ” ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ ì‹¬ì¸µì ì¸ ì‚¬ê³  ê³¼ì •ì„ ì„œìˆ í•´ ì£¼ì„¸ìš”. ë¨¼ì € íŒ©íŠ¸ë¥¼ ë‚˜ì—´í•˜ê³ , ì´ê²ƒì´ ê±°ì‹œê²½ì œ(ê¸ˆë¦¬, í™˜ìœ¨)ì™€ í•´ë‹¹ ì‚°ì—… ë°¸ë¥˜ì²´ì¸ì— ë¯¸ì¹  ì˜í–¥ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•´ ì£¼ì„¸ìš”. ì´ í•„ë“œëŠ” ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì§€ì§€ ì•Šì§€ë§Œ, ë’¤ì´ì–´ ë‚˜ì˜¬ ì „ë¬¸ê°€ìš©(Lv5) ë¶„ì„ì˜ ì§ˆì„ ë†’ì´ê¸° ìœ„í•œ ë¸Œë ˆì¸ìŠ¤í† ë° ê³µê°„ì…ë‹ˆë‹¤.",

  "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2", "í•µì‹¬í‚¤ì›Œë“œ3"],
  "sentiment_score": 75,
  "vocabulary": [
    {{"term": "ê¸°ì‚¬ì—_ë‚˜ì˜¨_ì–´ë ¤ìš´_ìš©ì–´", "definition": "í•´ë‹¹ ìš©ì–´ì— ëŒ€í•œ ì´ˆë³´ììš© í•´ì„¤ì…ë‹ˆë‹¤."}}
  ],

  "level_content": {{
    "lv1": {{
      "summary": "í•œ ë¬¸ì¥(1ì¤„) ìš”ì•½ì…ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ, ì´ëª¨ì§€, ë¶ˆë¦¿, ë²ˆí˜¸, ì¥ì‹ë¬¸ì ì—†ì´ ì‘ì„±í•´ ì£¼ì„¸ìš”.",
      "bullet_points": ["ì•„ì£¼ ì‰¬ìš´ í•µì‹¬ ìš”ì•½ 1ì…ë‹ˆë‹¤.", "ì•„ì£¼ ì‰¬ìš´ í•µì‹¬ ìš”ì•½ 2ì…ë‹ˆë‹¤.", "ì•„ì£¼ ì‰¬ìš´ í•µì‹¬ ìš”ì•½ 3ì…ë‹ˆë‹¤."],
      "what_is_this": ["ì´ ë‰´ìŠ¤ê°€ ë¬´ì—‡ì¸ì§€ ì‰½ê²Œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤.", "ì´ ë‰´ìŠ¤ì˜ ë°°ê²½ì„ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤."],
      "why_important": ["ì´ê²Œ ì™œ ì¤‘ìš”í•œì§€ ìƒí™œ ë°€ì°©í˜•ìœ¼ë¡œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤.", "ì¤‘ìš”í•œ ì´ìœ ë¥¼ ì¶”ê°€ë¡œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤."],
      "stock_impact": {{
        "positives": ["ì¢‹ì€ ì  1ì…ë‹ˆë‹¤.", "ì¢‹ì€ ì  2ì…ë‹ˆë‹¤."],
        "warnings": ["ì¡°ì‹¬í•  ì  1ì…ë‹ˆë‹¤.", "ì¡°ì‹¬í•  ì  2ì…ë‹ˆë‹¤."]
      }},
      "strategy_guide": {{
        "short_term": "ì£¼ë¦°ì´ë¥¼ ìœ„í•œ ë‹¨ê¸° ì¡°ì–¸ì…ë‹ˆë‹¤.",
        "long_term": "ì£¼ë¦°ì´ë¥¼ ìœ„í•œ ì¥ê¸° ì¡°ì–¸ì…ë‹ˆë‹¤."
      }},
      "action_guide": "ì£¼ë¦°ì´ë¥¼ ìœ„í•œ ì•„ì£¼ ê¸°ì´ˆì ì¸ ì¡°ì–¸ì…ë‹ˆë‹¤."
    }},
    "lv2": {{
      "summary": "í•œ ë¬¸ì¥(1ì¤„) ìš”ì•½ì…ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ, ì´ëª¨ì§€, ë¶ˆë¦¿, ë²ˆí˜¸, ì¥ì‹ë¬¸ì ì—†ì´ ì‘ì„±í•´ ì£¼ì„¸ìš”.",
      "bullet_points": ["ì‰¬ìš´ ìš”ì•½ 1ì…ë‹ˆë‹¤.", "ì‰¬ìš´ ìš”ì•½ 2ì…ë‹ˆë‹¤.", "ì‰¬ìš´ ìš”ì•½ 3ì…ë‹ˆë‹¤."],
      "what_is_this": ["ë‰´ìŠ¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤.", "ë°°ê²½ì„ ì¶”ê°€ë¡œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤."],
      "why_important": ["ì‹œì¥ì— ì¤‘ìš”í•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤.", "ì¤‘ìš”í•œ ì´ìœ ë¥¼ ì¶”ê°€ë¡œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤."],
      "stock_impact": {{
        "positives": ["ê¸ì •ì  ìš”ì¸ 1ì…ë‹ˆë‹¤.", "ê¸ì •ì  ìš”ì¸ 2ì…ë‹ˆë‹¤."],
        "warnings": ["ë¶€ì •ì  ìš”ì¸ 1ì…ë‹ˆë‹¤.", "ë¶€ì •ì  ìš”ì¸ 2ì…ë‹ˆë‹¤."]
      }},
      "strategy_guide": {{
        "short_term": "ì´ˆë³´ìë¥¼ ìœ„í•œ ë‹¨ê¸° ëŒ€ì‘ë²•ì…ë‹ˆë‹¤.",
        "long_term": "ì´ˆë³´ìë¥¼ ìœ„í•œ ì¥ê¸° íˆ¬ì ê´€ì ì…ë‹ˆë‹¤."
      }},
      "action_guide": "ì´ˆë³´ìë¥¼ ìœ„í•œ íˆ¬ì ì¡°ì–¸ì…ë‹ˆë‹¤."
    }},
    "lv3": {{
      "summary": "í•œ ë¬¸ì¥(1ì¤„) ìš”ì•½ì…ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ, ì´ëª¨ì§€, ë¶ˆë¦¿, ë²ˆí˜¸, ì¥ì‹ë¬¸ì ì—†ì´ ì‘ì„±í•´ ì£¼ì„¸ìš”.",
      "bullet_points": ["í•µì‹¬ ìš”ì•½ 1ì…ë‹ˆë‹¤.", "í•µì‹¬ ìš”ì•½ 2ì…ë‹ˆë‹¤.", "í•µì‹¬ ìš”ì•½ 3ì…ë‹ˆë‹¤."],
      "what_is_this": ["ì‹¬ë„ ìˆëŠ” ë‰´ìŠ¤ í•´ì„ì„ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤.", "ì‹¬ë„ ìˆëŠ” í•´ì„ì„ ì¶”ê°€ë¡œ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤."],
      "why_important": ["ì‚°ì—… ë° ì‹œì¥ ì˜í–¥ ë¶„ì„ì„ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤.", "ì˜í–¥ ë¶„ì„ì„ ì¶”ê°€ë¡œ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤."],
      "stock_impact": {{
        "positives": ["ìƒìŠ¹ ì¬ë£Œ 1ì…ë‹ˆë‹¤.", "ìƒìŠ¹ ì¬ë£Œ 2ì…ë‹ˆë‹¤."],
        "warnings": ["í•˜ë½ ë¦¬ìŠ¤í¬ 1ì…ë‹ˆë‹¤.", "í•˜ë½ ë¦¬ìŠ¤í¬ 2ì…ë‹ˆë‹¤."]
      }},
      "strategy_guide": {{
        "short_term": "ê¸°ìˆ ì  ë¶„ì„ì„ í¬í•¨í•œ ë‹¨ê¸° ì „ëµì…ë‹ˆë‹¤.",
        "long_term": "ì‚°ì—… ì‚¬ì´í´ì„ ê³ ë ¤í•œ ì¥ê¸° ì „ëµì…ë‹ˆë‹¤."
      }},
      "action_guide": "ì¤‘ê¸‰ìë¥¼ ìœ„í•œ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì • ì¡°ì–¸ì…ë‹ˆë‹¤."
    }},
    "lv4": {{
      "summary": "í•œ ë¬¸ì¥(1ì¤„) ìš”ì•½ì…ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ, ì´ëª¨ì§€, ë¶ˆë¦¿, ë²ˆí˜¸, ì¥ì‹ë¬¸ì ì—†ì´ ì‘ì„±í•´ ì£¼ì„¸ìš”.",
      "bullet_points": ["ì „ë¬¸ì  ìš”ì•½ 1ì…ë‹ˆë‹¤.", "ì „ë¬¸ì  ìš”ì•½ 2ì…ë‹ˆë‹¤.", "ì „ë¬¸ì  ìš”ì•½ 3ì…ë‹ˆë‹¤."],
      "what_is_this": ["êµ¬ì¡°ì /ì¬ë¬´ì  ê´€ì ì˜ ë¶„ì„ì„ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤.", "ë¶„ì„ì„ ì¶”ê°€ë¡œ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤."],
      "why_important": ["ë°¸ë¥˜ì²´ì¸ ë° ê±°ì‹œê²½ì œ ì˜í–¥ì„ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤.", "ì˜í–¥ì„ ì¶”ê°€ë¡œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤."],
      "stock_impact": {{
        "positives": ["í€ë”ë©˜í„¸ ê°œì„  ìš”ì¸ 1ì…ë‹ˆë‹¤.", "ìˆ˜ê¸‰/ëª¨ë©˜í…€ ìš”ì¸ 2ì…ë‹ˆë‹¤."],
        "warnings": ["ë°¸ë¥˜ì—ì´ì…˜ ë¶€ë‹´ 1ì…ë‹ˆë‹¤.", "ë¦¬ìŠ¤í¬ ìš”ì¸ 2ì…ë‹ˆë‹¤."]
      }},
      "strategy_guide": {{
        "short_term": "íŠ¸ë ˆì´ë”© ê´€ì ì˜ ë§¤ë§¤ ì „ëµì…ë‹ˆë‹¤.",
        "long_term": "ë°¸ë¥˜ì—ì´ì…˜ ë¦¬ë ˆì´íŒ… ê°€ëŠ¥ì„± ë¶„ì„ì…ë‹ˆë‹¤."
      }},
      "action_guide": "ìˆ™ë ¨ìë¥¼ ìœ„í•œ ë§¤ë§¤/í—¤ì§• ì „ëµì…ë‹ˆë‹¤."
    }},
    "lv5": {{
      "summary": "í•œ ë¬¸ì¥(1ì¤„) ìš”ì•½ì…ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ, ì´ëª¨ì§€, ë¶ˆë¦¿, ë²ˆí˜¸, ì¥ì‹ë¬¸ì ì—†ì´ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì—…ê³„ ì „ë¬¸ ìš©ì–´ë¥¼ ì ê·¹ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.",
      "bullet_points": ["Insightful Summary 1ì…ë‹ˆë‹¤.", "Insightful Summary 2ì…ë‹ˆë‹¤.", "Insightful Summary 3ì…ë‹ˆë‹¤."],
      "what_is_this": ["ì‹¬ì¸µ ë¶„ì„(Deep Dive)ì„ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤.", "ì‹¬ì¸µ ë¶„ì„ì„ ì¶”ê°€ë¡œ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤."],
      "why_important": ["Global Macro & Sector Impactë¥¼ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤.", "ì˜í–¥ì„ ì¶”ê°€ë¡œ ì„¤ëª…í•´ ë“œë¦½ë‹ˆë‹¤."],
      "stock_impact": {{
        "positives": ["Upside Potential Logic 1ì…ë‹ˆë‹¤.", "Catalyst 2ì…ë‹ˆë‹¤."],
        "warnings": ["Downside Risk 1ì…ë‹ˆë‹¤.", "Risk Factors 2ì…ë‹ˆë‹¤."]
      }},
      "strategy_guide": {{
        "short_term": "Arbitrage / Event-Driven Strategyë¥¼ ì œì•ˆí•´ ë“œë¦½ë‹ˆë‹¤.",
        "long_term": "Thematic / Structural Growth Thesisë¥¼ ì œì‹œí•´ ë“œë¦½ë‹ˆë‹¤."
      }},
      "action_guide": "ê¸°ê´€ íˆ¬ììê¸‰ì˜ High-Level ì „ëµì„ ì œì•ˆí•´ ë“œë¦½ë‹ˆë‹¤."
    }}
  }}
}}

[ì‘ì„± ì§€ì¹¨]
1) deep_analysis_reasoningì„ ê°€ì¥ ë¨¼ì € ì‘ì„±í•´ ì£¼ì„¸ìš”.
2) lv1~lv5ì˜ summaryëŠ” ë°˜ë“œì‹œ í•œ ë¬¸ì¥(1ì¤„)ë¡œ ì‘ì„±í•´ ì£¼ì‹œê³  ì¤„ë°”ê¿ˆ(\\n)ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.
3) summaryì—ëŠ” ë²ˆí˜¸/ë¶ˆë¦¿/ë‚˜ì—´(ì˜ˆ: "1) 2)", "-", "â€¢", "*") ë° ì´ëª¨ì§€/ì¥ì‹ë¬¸ìë¥¼ ë„£ì§€ ë§ì•„ ì£¼ì„¸ìš”.
4) lv1~lv5ëŠ” ê´€ì /ê¹Šì´ë¥¼ í™•ì‹¤íˆ ë‹¤ë¥´ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
5) sentiment_scoreëŠ” 0~100 ì •ìˆ˜ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
6) ëª¨ë“  ë¬¸ì¥ì€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
""".strip()


def _llm_chat(client, msgs: list[ChatMessage]) -> str:
    """
    gemini_clientì˜ chat ì‹œê·¸ë‹ˆì²˜ê°€ (msgs, use_search=...) í˜•íƒœë„ ìˆê³ , (msgs)ë§Œ ë˜ëŠ” í˜•íƒœë„ ìˆì–´ ë°©ì–´.
    """
    try:
        return client.chat(msgs, use_search=False)
    except TypeError:
        return client.chat(msgs)


# =========================================================
# Public API
# =========================================================
def analyze_trend_keyword_news(
    *,
    news: TrendKeywordNews,
    save_to_db: bool = True,
) -> Optional[Dict[str, Any]]:
    """
    TrendKeywordNews 1ê±´ì„ Geminië¡œ ë¶„ì„í•˜ê³ :
      - TrendKeywordNews.analysis_full
      - TrendKeywordNews.analyzed_at
      - TrendKeywordNewsAnalysis (lv1~lv5) upsert

    âœ… ë³€ê²½: ë ˆë²¨ë³„ summaryê°€ "ì´ëª¨í‹°ì½˜/ì¥ì‹ ì—†ì´ ê¹”ë”í•œ 1ì¤„"ë¡œ ìƒì„±ë˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ë§Œ ê°•í™”
    """
    content_to_analyze = (news.content or "").strip() or (news.summary or "").strip()
    if not content_to_analyze:
        return None

    client = get_gemini_client()

    prompt = _build_prompt(news.title, content_to_analyze)
    msgs = [
        ChatMessage(
            role="system",
            content=(
                "ë‹¹ì‹ ì€ JSONë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ì€ ê¸ˆì§€ì…ë‹ˆë‹¤. "
                "ëª¨ë“  ë¬¸ì¥ì€ ì¡´ëŒ“ë§ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. "
                "íŠ¹íˆ lv1~lv5 summaryëŠ” ì´ëª¨ì§€/ì¥ì‹ë¬¸ì/ë¶ˆë¦¿/ë²ˆí˜¸ ì—†ì´ í•œ ë¬¸ì¥(1ì¤„)ë¡œ ì‘ì„±í•©ë‹ˆë‹¤."
            ),
        ),
        ChatMessage(role="user", content=prompt),
    ]

    raw = _llm_chat(client, msgs)
    full = _safe_json_load(raw)
    if not full:
        return None

    full = _normalize_full(full)

    if not save_to_db:
        return full

    with transaction.atomic():
        news.analysis_full = full
        news.analyzed_at = timezone.now()
        news.save(update_fields=["analysis_full", "analyzed_at"])

        level_map = {1: "lv1", 2: "lv2", 3: "lv3", 4: "lv4", 5: "lv5"}
        for level, key in level_map.items():
            payload = _build_level_payload(full, key)
            TrendKeywordNewsAnalysis.objects.update_or_create(
                news=news,
                level=level,
                defaults={"analysis": payload},
            )

    return full
