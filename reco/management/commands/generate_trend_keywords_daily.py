# apps/reco/management/commands/generate_trend_keywords_daily.py
from __future__ import annotations

import json
import re
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Iterable, List, Optional
from urllib.parse import parse_qs, unquote, urljoin, urlparse, urlunparse
from zoneinfo import ZoneInfo

import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand
from django.core.management import call_command
from django.db import transaction
from django.utils import timezone

from ...models import TrendKeywordDaily, TrendKeywordNews, TrendScope
from main.services.gemini_client import get_gemini_client, ChatMessage


# =========================================================
# Config
# =========================================================
KEYWORD_LIMIT = 3

# âœ… ìµœì¢… ì €ì¥ ê°œìˆ˜(í‚¤ì›Œë“œë³„)
NEWS_LIMIT = 15

# âœ… í›„ë³´ í’€: í‚¤ì›Œë“œë³„ë¡œ ìµœëŒ€ 100ê°œê¹Œì§€ ëª¨ì€ í›„ ì„ ë³„
CANDIDATE_POOL_LIMIT = 100

# âœ… LLMì—ì„œ í•œ ë²ˆì— ìš”ì²­í•  ë‰´ìŠ¤ ê°œìˆ˜
BATCH_SIZE = 25

# âœ… í›„ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰ ë°˜ë³µ íšŸìˆ˜
MAX_REFILL_ATTEMPTS = 10

REQUEST_TIMEOUT = 8.0
KST = ZoneInfo("Asia/Seoul")

# âœ… â€œê³¼ê±° ë‰´ìŠ¤ ì ˆëŒ€ ì•ˆë¨â€
MAX_AGE_DAYS = 4

# âœ… ë³¸ë¬¸ ì €ì¥ ìµœëŒ€ ê¸¸ì´
CONTENT_MAX_CHARS = 6000

# âœ… ê¸°ì‚¬ ë³¸ë¬¸ ìµœì†Œ ê¸¸ì´(ë„ˆë¬´ ì§§ìœ¼ë©´ ëª©ë¡/ë©”ì¸/ì¤‘ê³„ì¼ í™•ë¥  ë†’ìŒ)
MIN_ARTICLE_TEXT_CHARS = 180

BLOCKED_DOMAINS = {
    "example.com",
    "vertexaisearch.cloud.google.com",
    "webcache.googleusercontent.com",
    # ê²€ìƒ‰/ì¤‘ê³„ ë¥˜(í•„ìš” ì‹œ í™•ì¥)
    "news.google.com",
}

BLOCKED_HOST_KEYWORDS = (
    "vertexaisearch",
    "example.com",
)

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}


# =========================================================
# Prompt
# =========================================================
TREND_JSON_INSTRUCTION = f"""
ë„ˆëŠ” í˜„ì¬ ì‹œê° ê¸°ì¤€ ì£¼ì‹/ê¸ˆìœµ ì‹œì¥ì˜ ì‹¤ì‹œê°„ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ëŠ” AI ì—”ì§„ì´ë‹¤.
Google Search(ì‹¤ì‹œê°„ ê²€ìƒ‰)ë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µí•´ë¼.

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ë§Œ í—ˆìš©í•œë‹¤. (ë§ˆí¬ë‹¤ìš´, ì½”ë“œë¸”ë¡ ê¸ˆì§€)

{{
  "items": [
    {{
      "keyword": "í‚¤ì›Œë“œ(5ê¸€ìì´ë‚´)",
      "reason": "ì„ ì • ì´ìœ (2ë¬¸ì¥ ì´ë‚´, ë¦¬ìŠ¤í¬ 1ê°œ í¬í•¨)",
      "news": [
        {{
          "title": "ë‰´ìŠ¤ ì œëª©",
          "summary": "ë‰´ìŠ¤ ìš”ì•½(1ë¬¸ì¥)",
          "link": "ì‹¤ì œ ê¸°ì‚¬ URL (ì¤‘ê³„/placeholder ê¸ˆì§€)",
          "image_url": "ì´ë¯¸ì§€ URL (ì—†ìœ¼ë©´ ë¹ˆë¬¸ìì—´)",
          "published_at": "ë°œí–‰ì¼ì‹œ(YYYY-MM-DD HH:MM, KST ê¶Œì¥)"
        }}
      ]
    }}
  ]
}}

[ì ˆëŒ€ ê·œì¹™]
- items ê°œìˆ˜ëŠ” ì •í™•íˆ {KEYWORD_LIMIT}ê°œ.
- keywordëŠ” ê³µë°± í¬í•¨ ìµœëŒ€ 5ê¸€ì(ë˜ëŠ” ë§¤ìš° ì§§ê²Œ).
- newsëŠ” ê° í‚¤ì›Œë“œë‹¹ ìµœì†Œ {BATCH_SIZE}ê°œ ì´ìƒì„ ì œê³µí•˜ë ¤ê³  ë…¸ë ¥í•´ë¼(ë¶€ì¡±í•˜ë©´ ìµœëŒ€í•œ).
- newsì˜ linkëŠ” ë°˜ë“œì‹œ ê°€ì ¸ì™€ì„œ ìš”ì•½í•œ ì‹¤ì œ ê¸°ì‚¬ URLì´ì–´ì•¼ í•œë‹¤.
  - example.com ê°™ì€ placeholder ê¸ˆì§€
  - vertexaisearch.cloud.google.com ê°™ì€ ì¤‘ê³„ URL ê¸ˆì§€
- published_atì€ ê°€ëŠ¥í•˜ë©´ 'YYYY-MM-DD HH:MM' í˜•ì‹(KST)ìœ¼ë¡œ ì±„ì›Œë¼.

[ìµœì‹ ì„± ê°•ì œ]
- newsëŠ” "ì˜¤ëŠ˜(KST) ë˜ëŠ” ìµœê·¼ {MAX_AGE_DAYS}ì¼ ì´ë‚´(KST)" ê¸°ì‚¬ë§Œ í—ˆìš©í•œë‹¤. (ê·¸ ì´ì „ ê¸ˆì§€)
- ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ë§¤ì²´ì˜ ìµœì‹  ê¸°ì‚¬ë¡œ ë‹¤ì‹œ ì°¾ì•„ ì±„ì›Œë¼.
""".strip()


def _now_kst() -> datetime:
    return timezone.now().astimezone(KST)


def _build_user_msg(scope: str, now_kst: datetime) -> str:
    scope = (scope or "").strip().upper()
    base = (
        "Google Search ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ 'í˜„ì¬ ì‹œê°„(Real-time)'ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ë¼.\n"
        f"í˜„ì¬ KST ì‹œê°: {now_kst.strftime('%Y-%m-%d %H:%M')}\n"
        f"ì¡°ê±´: ë°˜ë“œì‹œ ì˜¤ëŠ˜ ë˜ëŠ” ìµœê·¼ {MAX_AGE_DAYS}ì¼ ì´ë‚´(KST) ê¸°ì‚¬ë§Œ ì‚¬ìš©.\n"
        "ì¡°ê±´: linkëŠ” ì‹¤ì œ ê¸°ì‚¬ URLë§Œ. example.com/vertexaisearch ë“± ê¸ˆì§€.\n"
        "ì¡°ê±´: published_atì€ YYYY-MM-DD HH:MM(KST)ë¡œ ì¶œë ¥.\n"
    )

    if scope == TrendScope.KR:
        target = "í•œêµ­(KR) ì£¼ì‹ ì‹œì¥ ë° ê²½ì œ"
        ratio = f"í‚¤ì›Œë“œ {KEYWORD_LIMIT}ê°œ ëª¨ë‘ í•œêµ­ ê´€ë ¨ ì´ìŠˆë¡œ ì„ ì •."
    else:
        target = "ë¯¸êµ­(US) ì£¼ì‹ ì‹œì¥ ë° ê²½ì œ"
        ratio = f"í‚¤ì›Œë“œ {KEYWORD_LIMIT}ê°œ ëª¨ë‘ ë¯¸êµ­ ê´€ë ¨ ì´ìŠˆë¡œ ì„ ì •."

    return f"""{base}
ëŒ€ìƒ ì‹œì¥: {target}
ìš”ì²­ ì‚¬í•­: {ratio}
ê° í‚¤ì›Œë“œë§ˆë‹¤ ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ëª©ë¡ì„ ìµœëŒ€í•œ ë§ì´(ìµœì†Œ {BATCH_SIZE}ê°œ ëª©í‘œ) ì±„ì›Œë¼.
""".strip()


def _build_keyword_refill_msg(
    scope: str,
    keyword: str,
    now_kst: datetime,
    exclude_urls: Iterable[str],
    batch_size: int,
) -> str:
    scope = (scope or "").strip().upper()
    target = "í•œêµ­(KR)" if scope == TrendScope.KR else "ë¯¸êµ­(US)"
    excl = "\n".join(f"- {u}" for u in list(exclude_urls)[:80])

    return f"""
Google Search ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ë¼.
í˜„ì¬ KST ì‹œê°: {now_kst.strftime('%Y-%m-%d %H:%M')}

[ëª©í‘œ]
í‚¤ì›Œë“œ: "{keyword}" (ëŒ€ìƒ ì‹œì¥: {target})
newsë¥¼ ìµœì†Œ {batch_size}ê°œ ì´ìƒ ë°˜í™˜í•˜ë ¤ê³  ë…¸ë ¥í•´ë¼.
ë°˜ë“œì‹œ ì˜¤ëŠ˜ ë˜ëŠ” ìµœê·¼ {MAX_AGE_DAYS}ì¼ ì´ë‚´(KST) ê¸°ì‚¬ë§Œ í—ˆìš©.
linkëŠ” ì‹¤ì œ ê¸°ì‚¬ URLë§Œ í—ˆìš©(placeholder/ì¤‘ê³„ URL ê¸ˆì§€).
published_atì€ YYYY-MM-DD HH:MM(KST)ë¡œ ì¶œë ¥.

[ì´ë¯¸ ì‚¬ìš©í•œ URL - ì¤‘ë³µ ê¸ˆì§€]
{excl if excl else "(ì—†ìŒ)"}

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ JSONë§Œ:
{{
  "news": [
    {{
      "title": "ë‰´ìŠ¤ ì œëª©",
      "summary": "ë‰´ìŠ¤ ìš”ì•½(1ë¬¸ì¥)",
      "link": "ì‹¤ì œ ê¸°ì‚¬ URL",
      "image_url": "ì´ë¯¸ì§€ URL(ì—†ìœ¼ë©´ ë¹ˆë¬¸ìì—´)",
      "published_at": "YYYY-MM-DD HH:MM"
    }}
  ]
}}
""".strip()


# =========================================================
# JSON helpers
# =========================================================
def _safe_json_load(s: str) -> dict:
    s = (s or "").strip()
    if not s:
        return {}

    if s.startswith("```"):
        lines = s.splitlines()
        if lines and lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].startswith("```"):
            lines = lines[:-1]
        s = "\n".join(lines).strip()

    l = s.find("{")
    r = s.rfind("}")
    if l >= 0 and r >= 0 and r > l:
        s = s[l : r + 1]

    try:
        return json.loads(s)
    except Exception:
        return {}


def _sanitize_keyword(s: Any) -> str:
    kw = str(s or "").strip()
    if len(kw) > 7:
        kw = kw[:7]
    return kw


def _sanitize_text(s: Any, limit: int) -> str:
    return str(s or "")[:limit]


# =========================================================
# De-dup helpers (URL + Title)
# =========================================================
_TITLE_TRIM_PREFIX = re.compile(r"^\s*(\[[^\]]+\]|\([^)]+\)|<[^>]+>|[0-9]+[.)\]]\s*)\s*")
_TITLE_TRIM_SUFFIX = re.compile(r"\s*[-â€“â€”]\s*[^-â€“â€”]{1,25}\s*$")  # ëì˜ ë§¤ì²´ëª…/ê¸°ìëª… ë¥˜


def _normalize_title(title: str) -> str:
    """
    ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±°ìš© ì •ê·œí™”:
    - [ì†ë³´], (ì¢…í•©) ê°™ì€ prefix ì œê±°
    - ëì˜ "- ì¡°ì„ ì¼ë³´" ê°™ì€ suffix ì œê±°(ëŒ€ëµì ì¸ íœ´ë¦¬ìŠ¤í‹±)
    - ê³µë°± ì •ë¦¬, ì†Œë¬¸ìí™”
    """
    t = (title or "").strip()
    if not t:
        return ""
    t = _TITLE_TRIM_PREFIX.sub("", t).strip()
    t = _TITLE_TRIM_SUFFIX.sub("", t).strip()
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t[:160]


# =========================================================
# URL validation + canonicalize (HARDENED)
# =========================================================
def _is_http_url(url: str) -> bool:
    try:
        u = urlparse(url)
        return u.scheme in ("http", "https") and bool(u.netloc)
    except Exception:
        return False


def _is_blocked_url(url: str) -> bool:
    if not _is_http_url(url):
        return True
    host = (urlparse(url).netloc or "").lower().replace("www.", "")
    if host in BLOCKED_DOMAINS:
        return True
    if any(k in host for k in BLOCKED_HOST_KEYWORDS):
        return True
    return False


_REDIRECT_PARAM_KEYS = ("url", "u", "q", "target", "dest", "destination", "redirect", "redir")

# ì„¹ì…˜/ëª©ë¡/ë©”ì¸/ë­í‚¹ í˜ì´ì§€ë¡œ ìì£¼ ë³´ì´ëŠ” path í‚¤ì›Œë“œ(ë„ë©”ì¸ ê³µí†µ íœ´ë¦¬ìŠ¤í‹±)
_NON_ARTICLE_PATH_HINTS = (
    "/index",
    "/main",
    "/home",
    "/all",
    "/list",
    "/lists",
    "/section",
    "/sections",
    "/category",
    "/categories",
    "/market_cap",
    "/volume",
    "/rise_stocks",
    "/fall_stocks",
)


def _strip_fragment(url: str) -> str:
    try:
        u = urlparse(url)
        return urlunparse((u.scheme, u.netloc, u.path, u.params, u.query, ""))  # fragment ì œê±°
    except Exception:
        return (url or "").strip()


def _unwrap_redirect_url(url: str) -> str:
    """
    1ì°¨: ë¦¬ë‹¤ì´ë ‰í„° URLì—ì„œ ì‹¤ì œ URLì´ query paramìœ¼ë¡œ ë“¤ì–´ìˆëŠ” ê²½ìš°ë¥¼ ì–¸ë©.
    """
    u = (url or "").strip()
    if not _is_http_url(u):
        return u

    try:
        pu = urlparse(u)
        qs = parse_qs(pu.query)
        for k in _REDIRECT_PARAM_KEYS:
            vals = qs.get(k)
            if not vals:
                continue
            cand = (vals[0] or "").strip()
            cand = unquote(cand)
            if _is_http_url(cand):
                return cand
    except Exception:
        pass

    return u


def _extract_canonical_url_from_html(html: str, base_url: str) -> str:
    """
    HTMLì—ì„œ canonical/og:url ì¶”ì¶œ.
    """
    try:
        soup = BeautifulSoup(html, "html.parser")

        link = soup.find("link", attrs={"rel": re.compile(r"\bcanonical\b", re.I)})
        if link and link.get("href"):
            href = (link["href"] or "").strip()
            if href.startswith("/"):
                href = urljoin(base_url, href)
            if _is_http_url(href):
                return href

        meta = soup.find("meta", attrs={"property": "og:url"})
        if meta and meta.get("content"):
            href = (meta["content"] or "").strip()
            if href.startswith("/"):
                href = urljoin(base_url, href)
            if _is_http_url(href):
                return href
    except Exception:
        pass

    return ""


def _looks_like_article_url(url: str) -> bool:
    """
    URLì´ 'ê¸°ì‚¬'ë¡œ ë³´ì´ëŠ”ì§€ íœ´ë¦¬ìŠ¤í‹± ê²€ì‚¬.
    - pathê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì„¹ì…˜/ëª©ë¡ íŒíŠ¸ê°€ ìˆìœ¼ë©´ False
    - ë‚ ì§œ/ê¸´ ìˆ«ì(id) íŒ¨í„´ì´ ìˆìœ¼ë©´ True ê°€ì‚°
    """
    try:
        pu = urlparse(url)
        path = (pu.path or "").lower()
        if not path or path in ("/", ""):
            return False

        for hint in _NON_ARTICLE_PATH_HINTS:
            if hint in path:
                return False

        # ë„ˆë¬´ ì§§ì€ pathëŠ” ì„¹ì…˜ì¼ ê°€ëŠ¥ì„±
        if len(path.strip("/").split("/")) <= 1 and len(path) < 18:
            return False

        # ê¸°ì‚¬ ID/ë‚ ì§œ ë¥˜ê°€ ìˆìœ¼ë©´ ê¸°ì‚¬ì¼ í™•ë¥  ì¦ê°€
        if re.search(r"\b(20\d{2}[./-]\d{1,2}[./-]\d{1,2})\b", path):
            return True
        if re.search(r"\b\d{6,}\b", path):
            return True

        # ìœ„ì—ì„œ ì„¹ì…˜ë¥˜ëŠ” ê±¸ë €ìœ¼ë¯€ë¡œ ê¸°ë³¸ True
        return True
    except Exception:
        return False


def _finalize_article_url(url: str) -> tuple[str, Optional[str]]:
    """
    URLì„ 'ì •ì‹ ê¸°ì‚¬ URL'ë¡œ í™•ì •:
    1) redirect param ì–¸ë©
    2) GETìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë”°ë¼ final url í™•ë³´
    3) HTMLì—ì„œ canonical/og:urlë¡œ ìµœì¢… í™•ì •
    ë°˜í™˜: (final_url, html_or_none)
    """
    u0 = _unwrap_redirect_url(url)
    if not _is_http_url(u0):
        return u0, None

    try:
        with requests.Session() as s:
            r = s.get(
                u0,
                headers=DEFAULT_HEADERS,
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
            )
            final_url = str(r.url or u0)
            final_url = _strip_fragment(final_url)

            ct = (r.headers.get("Content-Type") or "").lower()
            html = r.text if (r.status_code == 200 and "text/html" in ct) else None

            if html:
                canon = _extract_canonical_url_from_html(html, base_url=final_url)
                canon = _strip_fragment(canon)
                if canon and _is_http_url(canon):
                    return canon, html

            return final_url, html
    except Exception:
        return u0, None


def _canonicalize_article_url(url: str) -> str:
    final_url, _html = _finalize_article_url(url)
    return final_url


# =========================================================
# Time parsing / recency
# =========================================================
def _parse_datetime_any(s: str) -> Optional[datetime]:
    t = (s or "").strip()
    if not t:
        return None

    m = re.search(r"\b(\d{4}-\d{2}-\d{2})[ T](\d{2}:\d{2})\b", t)
    if m:
        try:
            return datetime.fromisoformat(f"{m.group(1)} {m.group(2)}").replace(tzinfo=KST)
        except Exception:
            pass

    iso = t.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(iso)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        return dt.astimezone(KST)
    except Exception:
        pass

    m = re.search(r"\b(\d{4}-\d{2}-\d{2})\b", t)
    if m:
        try:
            return datetime.fromisoformat(m.group(1)).replace(tzinfo=KST, hour=12, minute=0)
        except Exception:
            return None

    return None


def _format_kst_min(dt: datetime) -> str:
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")


def _is_recent_kst(dt: datetime, now_kst: datetime) -> bool:
    d = (now_kst.date() - dt.astimezone(KST).date()).days
    return 0 <= d <= MAX_AGE_DAYS


# =========================================================
# Fetch HTML + parse published time / content / og image
# =========================================================
_PUB_META_KEYS = (
    ("meta", {"property": "article:published_time"}),
    ("meta", {"property": "og:published_time"}),
    ("meta", {"property": "article:modified_time"}),
    ("meta", {"property": "og:updated_time"}),
    ("meta", {"name": "pubdate"}),
    ("meta", {"name": "publishdate"}),
    ("meta", {"name": "timestamp"}),
    ("meta", {"name": "date"}),
)


def _fetch_html(url: str) -> Optional[str]:
    try:
        r = requests.get(url, headers=DEFAULT_HEADERS, timeout=REQUEST_TIMEOUT, allow_redirects=True)
        if r.status_code != 200:
            return None
        ct = (r.headers.get("Content-Type") or "").lower()
        if "text/html" not in ct:
            return None
        return r.text
    except Exception:
        return None


def _extract_published_at_from_html(html: str) -> Optional[datetime]:
    soup = BeautifulSoup(html, "html.parser")

    for tag_name, attrs in _PUB_META_KEYS:
        tag = soup.find(tag_name, attrs=attrs)
        if tag and tag.get("content"):
            dt = _parse_datetime_any(tag.get("content", ""))
            if dt:
                return dt

    for t in soup.find_all("time")[:5]:
        dt_attr = t.get("datetime") or ""
        dt = _parse_datetime_any(dt_attr)
        if dt:
            return dt
        dt = _parse_datetime_any(t.get_text(" ").strip())
        if dt:
            return dt

    return None


def _extract_og_image_from_html(html: str, base_url: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    candidates: List[str] = []

    tag = soup.find("meta", attrs={"property": "og:image"})
    if tag and tag.get("content"):
        candidates.append(tag["content"])

    tag = soup.find("meta", attrs={"name": "twitter:image"})
    if tag and tag.get("content"):
        candidates.append(tag["content"])

    tag = soup.find("meta", attrs={"name": "twitter:image:src"})
    if tag and tag.get("content"):
        candidates.append(tag["content"])

    for img in candidates:
        img = (img or "").strip()
        if not img:
            continue
        if img.startswith("/"):
            img = urljoin(base_url, img)
        return img
    return ""


def _clean_text(s: str) -> str:
    t = re.sub(r"\s+", " ", (s or "").strip())
    return t


def _extract_article_text_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    for tag in soup(["script", "style", "noscript", "header", "footer", "aside", "nav"]):
        tag.decompose()

    for sel in ["article", "main"]:
        node = soup.select_one(sel)
        if node:
            ps = [p.get_text(" ", strip=True) for p in node.find_all("p")]
            text = _clean_text(" ".join(ps))
            if len(text) >= 200:
                return text

    candidates_sel = [
        "#articleBody",
        "#article_body",
        "#newsct_article",
        "#content",
        "#contents",
        ".article-body",
        ".articleBody",
        ".news_body",
        ".newsBody",
        ".story-body",
        ".entry-content",
        ".post-content",
        ".post_body",
    ]
    for sel in candidates_sel:
        node = soup.select_one(sel)
        if node:
            ps = [p.get_text(" ", strip=True) for p in node.find_all("p")]
            text = _clean_text(" ".join(ps)) or _clean_text(node.get_text(" ", strip=True))
            if len(text) >= 200:
                return text

    ps = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
    text = _clean_text(" ".join(ps))
    return text


def _resolve_published_at_kst_min(article_url: str, candidate: str) -> Optional[str]:
    dt = _parse_datetime_any(candidate)
    if dt:
        return _format_kst_min(dt)

    html = _fetch_html(article_url)
    if html:
        dt2 = _extract_published_at_from_html(html)
        if dt2:
            return _format_kst_min(dt2)

    return None


# =========================================================
# Image resolution
# =========================================================
def _is_valid_image_url(url: str, timeout: float = 4.0) -> bool:
    url = (url or "").strip()
    if not _is_http_url(url):
        return False

    try:
        r = requests.head(url, headers=DEFAULT_HEADERS, timeout=timeout, allow_redirects=True)
        ct = (r.headers.get("Content-Type") or "").lower()
        if r.status_code == 200 and ct.startswith("image/"):
            return True
    except Exception:
        pass

    try:
        r = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout, allow_redirects=True, stream=True)
        ct = (r.headers.get("Content-Type") or "").lower()
        return r.status_code == 200 and ct.startswith("image/")
    except Exception:
        return False


def _fallback_favicon(article_url: str) -> str:
    try:
        host = urlparse(article_url).netloc
        if not host:
            return ""
        return f"https://www.google.com/s2/favicons?domain={host}&sz=128"
    except Exception:
        return ""


def _resolve_image_url(article_link: str, candidate_image_url: str, html: Optional[str]) -> tuple[str, bool]:
    candidate = (candidate_image_url or "").strip()
    if candidate and _is_valid_image_url(candidate):
        return candidate, False

    if html:
        og = _extract_og_image_from_html(html, base_url=article_link)
        if og and _is_valid_image_url(og):
            return og, False

    fav = _fallback_favicon(article_link)
    if fav:
        return fav, False

    return "", True


# =========================================================
# Normalize / rank
# =========================================================
@dataclass
class NewsNorm:
    title: str
    summary: str
    link: str
    image_url: str
    published_dt: datetime
    published_at: str  # YYYY-MM-DD HH:MM
    needs_image_gen: bool
    content: str
    normalized_title: str

    @property
    def has_image(self) -> bool:
        return bool(self.image_url) and not self.image_url.startswith("https://www.google.com/s2/favicons")


def _normalize_news_item(n: dict, now_kst: datetime) -> Optional[NewsNorm]:
    link_raw = (n.get("link") or "").strip()
    if not link_raw:
        return None

    # âœ… 1) ì •ì‹ URL í™•ì • + HTML 1íšŒ í™•ë³´
    link, html = _finalize_article_url(link_raw)
    link = (link or "").strip()
    if not link:
        return None

    # âœ… 2) ë¸”ë½/ì¤‘ê³„ ë„ë©”ì¸ ì œê±°
    if _is_blocked_url(link):
        return None

    # âœ… 3) ê¸°ì‚¬ URL í˜•íƒœ íœ´ë¦¬ìŠ¤í‹±(ì„¹ì…˜/ëª©ë¡/ë©”ì¸ ì œê±°)
    if not _looks_like_article_url(link):
        return None

    title = _sanitize_text(n.get("title"), 300).strip()
    summary = _sanitize_text(n.get("summary"), 1000).strip()

    # âœ… 4) ë°œí–‰ì‹œê° í™•ì •(í›„ë³´ -> HTML meta/time fallback)
    pub_str = _resolve_published_at_kst_min(link, _sanitize_text(n.get("published_at"), 100))
    if not pub_str:
        return None

    dt = _parse_datetime_any(pub_str)
    if not dt:
        return None

    if not _is_recent_kst(dt, now_kst):
        return None

    # âœ… 5) ì½˜í…ì¸  í™•ë³´ (ì´ë¯¸ htmlì„ ë°›ì•˜ìœ¼ë©´ ì¬ì‚¬ìš©)
    if not html:
        html = _fetch_html(link)

    img_url, needs_gen = _resolve_image_url(link, str(n.get("image_url") or ""), html=html)

    content = ""
    if html:
        content = _extract_article_text_from_html(html)
        content = content[:CONTENT_MAX_CHARS]

    # âœ… 6) ê¸°ì‚¬ì„± ìµœì¢… ê²€ì¦: ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ëª©ë¡/ë©”ì¸ì¼ í™•ë¥  ë†’ìŒ
    if len((content or "").strip()) < MIN_ARTICLE_TEXT_CHARS:
        return None

    nt = _normalize_title(title)

    return NewsNorm(
        title=title,
        summary=summary,
        link=link[:1000],
        image_url=(img_url or "")[:1000],
        published_dt=dt.astimezone(KST),
        published_at=_format_kst_min(dt),
        needs_image_gen=needs_gen,
        content=content or "",
        normalized_title=nt,
    )


def _collect_candidates(
    now_kst: datetime,
    raw_news_batches: Iterable[List[dict]],
    used_urls: set[str],
    used_titles: set[str],
    pool_limit: int,
) -> List[NewsNorm]:
    """
    í›„ë³´ ìˆ˜ì§‘ ì‹œì ì—ì„œ 1ì°¨ ì¤‘ë³µ ì œê±°:
    - canonical URL ì¤‘ë³µ ì œê±°
    - normalized title ì¤‘ë³µ ì œê±°
    """
    out: List[NewsNorm] = []
    for batch in raw_news_batches:
        for n in batch:
            norm = _normalize_news_item(n, now_kst)
            if not norm:
                continue

            if norm.link in used_urls:
                continue
            if norm.normalized_title and norm.normalized_title in used_titles:
                continue

            used_urls.add(norm.link)
            if norm.normalized_title:
                used_titles.add(norm.normalized_title)

            out.append(norm)
            if len(out) >= pool_limit:
                return out
    return out


def _rank_and_pick(
    cands: List[NewsNorm],
    limit: int,
    global_seen_urls: set[str],
    global_seen_titles: set[str],
) -> List[NewsNorm]:
    """
    1) ìµœì‹ ìˆœ(published_dt desc)
    2) ì´ë¯¸ì§€ ìˆëŠ” ê¸°ì‚¬ ìš°ì„ 
    3) scope ì „ì—­ ì¤‘ë³µ ì œê±°(ê°™ì€ ê¸°ì‚¬ê°€ ë‹¤ë¥¸ í‚¤ì›Œë“œì— ë˜ ë‚˜ì˜¤ì§€ ì•Šê²Œ)
    """
    if not cands:
        return []

    cands_sorted = sorted(cands, key=lambda x: x.published_dt, reverse=True)

    def take_unique(src: List[NewsNorm], need: int) -> List[NewsNorm]:
        picked: List[NewsNorm] = []
        for x in src:
            if x.link in global_seen_urls:
                continue
            if x.normalized_title and x.normalized_title in global_seen_titles:
                continue

            picked.append(x)
            global_seen_urls.add(x.link)
            if x.normalized_title:
                global_seen_titles.add(x.normalized_title)

            if len(picked) >= need:
                break
        return picked

    with_img = [x for x in cands_sorted if x.has_image]
    without_img = [x for x in cands_sorted if not x.has_image]

    picked = take_unique(with_img, limit)
    if len(picked) < limit:
        picked.extend(take_unique(without_img, limit - len(picked)))

    return picked[:limit]


def _final_dedupe_for_save(picked: List[NewsNorm]) -> List[NewsNorm]:
    """
    ì €ì¥ ì§ì „ ìµœì¢… ë°©ì–´ ì¤‘ë³µ ì œê±°(í‚¤ì›Œë“œ ë‚´ë¶€ì—ì„œ í˜¹ì‹œ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” ì¤‘ë³µ ì œê±°).
    """
    out: List[NewsNorm] = []
    seen_u: set[str] = set()
    seen_t: set[str] = set()
    for x in picked:
        if x.link in seen_u:
            continue
        nt = x.normalized_title
        if nt and nt in seen_t:
            continue
        seen_u.add(x.link)
        if nt:
            seen_t.add(nt)
        out.append(x)
    return out


# =========================================================
# LLM calls
# =========================================================
def _llm_chat(client, msgs: List[ChatMessage]) -> str:
    try:
        return client.chat(msgs, use_search=True)
    except TypeError:
        return client.chat(msgs)


def _refill_news_for_keyword(
    client,
    scope: str,
    keyword: str,
    now_kst: datetime,
    exclude_urls: set[str],
    batch_size: int,
) -> List[dict]:
    msg = _build_keyword_refill_msg(
        scope=scope,
        keyword=keyword,
        now_kst=now_kst,
        exclude_urls=exclude_urls,
        batch_size=batch_size,
    )
    msgs = [
        ChatMessage(role="system", content="ë„ˆëŠ” JSONë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€."),
        ChatMessage(role="user", content=msg),
    ]
    raw = _llm_chat(client, msgs)
    data = _safe_json_load(raw)
    news = data.get("news") or []
    return news if isinstance(news, list) else []


# =========================================================
# DB save
# =========================================================
def _save_to_db(today_date, scope: str, items: list[dict]) -> int:
    with transaction.atomic():
        TrendKeywordDaily.objects.filter(date=today_date, scope=scope).delete()

        for rank, it in enumerate(items, start=1):
            kw_obj = TrendKeywordDaily.objects.create(
                date=today_date,
                scope=scope,
                rank=rank,
                keyword=it["keyword"],
                reason=it["reason"],
            )

            news_objs: List[TrendKeywordNews] = []
            picked: List[NewsNorm] = it.get("picked_news", [])[:NEWS_LIMIT]
            picked = _final_dedupe_for_save(picked)

            for n in picked:
                news_objs.append(
                    TrendKeywordNews(
                        trend=kw_obj,
                        title=n.title,
                        summary=n.summary,
                        content=(n.content or "")[:CONTENT_MAX_CHARS],
                        link=n.link,
                        image_url=n.image_url,
                        published_at=n.published_at,  # YYYY-MM-DD HH:MM
                        needs_image_gen=n.needs_image_gen,
                    )
                )

            if news_objs:
                TrendKeywordNews.objects.bulk_create(news_objs)

    return len(items)


# =========================================================
# Management Command
# =========================================================
class Command(BaseCommand):
    help = (
        "Generate trend keywords (KR/US 3 each) and news per keyword: "
        "collect up to 100 candidates, de-dup by url/title, pick newest with images, "
        "and de-dup across keywords per scope; save up to 15 incl. content. "
        "After saving, auto-runs analyze_trend_keyword_news with no args."
    )

    def handle(self, *args, **opts):
        scopes = [TrendScope.KR, TrendScope.US]
        now_kst = _now_kst()
        today = now_kst.date()

        client = get_gemini_client()

        for scope in scopes:
            self.stdout.write(f"Requesting {scope} trends with Google Search...")

            # âœ… scope ì „ì—­ ì¤‘ë³µ ì œê±° ì„¸íŠ¸(í‚¤ì›Œë“œ ê°„ ì¤‘ë³µ ë°©ì§€)
            global_seen_urls: set[str] = set()
            global_seen_titles: set[str] = set()

            user_msg = _build_user_msg(scope, now_kst=now_kst)
            msgs = [
                ChatMessage(role="system", content=TREND_JSON_INSTRUCTION),
                ChatMessage(role="user", content=user_msg),
            ]

            raw = _llm_chat(client, msgs)
            data = _safe_json_load(raw)
            items_raw = data.get("items") or []
            if not isinstance(items_raw, list):
                items_raw = []

            items: List[dict] = []
            for x in items_raw[:KEYWORD_LIMIT]:
                if not isinstance(x, dict):
                    continue
                items.append(
                    {
                        "keyword": _sanitize_keyword(x.get("keyword")),
                        "reason": _sanitize_text(x.get("reason"), 2000),
                        "news_seed": x.get("news") if isinstance(x.get("news"), list) else [],
                    }
                )

            while len(items) < KEYWORD_LIMIT:
                items.append({"keyword": "N/A", "reason": "ë°ì´í„° ì—†ìŒ", "news_seed": []})

            # í‚¤ì›Œë“œë³„ í›„ë³´ 100ê°œ ìˆ˜ì§‘ -> ìµœì‹  + ì´ë¯¸ì§€ ìš°ì„  15ê°œ ì €ì¥(+ë³¸ë¬¸)
            for it in items:
                kw = it["keyword"]

                # âœ… í‚¤ì›Œë“œ í›„ë³´ ìˆ˜ì§‘ ë‹¨ê³„ì˜ ì¤‘ë³µ ì œê±°(í‚¤ì›Œë“œ ë‚´ë¶€)
                used_urls: set[str] = set()
                used_titles: set[str] = set()

                raw_batches: List[List[dict]] = [it.get("news_seed") or []]

                candidates = _collect_candidates(
                    now_kst=now_kst,
                    raw_news_batches=raw_batches,
                    used_urls=used_urls,
                    used_titles=used_titles,
                    pool_limit=CANDIDATE_POOL_LIMIT,
                )

                attempts = 0
                while len(candidates) < CANDIDATE_POOL_LIMIT and attempts < MAX_REFILL_ATTEMPTS:
                    attempts += 1
                    refill = _refill_news_for_keyword(
                        client=client,
                        scope=scope,
                        keyword=kw,
                        now_kst=now_kst,
                        exclude_urls=used_urls,  # URL ìœ„ì£¼ë¡œë§Œ exclude ì „ë‹¬ (LLMìš©)
                        batch_size=BATCH_SIZE,
                    )
                    if not refill:
                        continue

                    new_cands = _collect_candidates(
                        now_kst=now_kst,
                        raw_news_batches=[refill],
                        used_urls=used_urls,
                        used_titles=used_titles,
                        pool_limit=(CANDIDATE_POOL_LIMIT - len(candidates)),
                    )
                    candidates.extend(new_cands)

                    if attempts >= 3 and len(new_cands) == 0:
                        break

                # âœ… pick ë‹¨ê³„ì—ì„œ "scope ì „ì—­ ì¤‘ë³µ"ê¹Œì§€ ì œê±°
                picked = _rank_and_pick(
                    cands=candidates,
                    limit=NEWS_LIMIT,
                    global_seen_urls=global_seen_urls,
                    global_seen_titles=global_seen_titles,
                )

                # âœ… ì €ì¥ ì§ì „ ë°©ì–´ ì¤‘ë³µ ì œê±°
                picked = _final_dedupe_for_save(picked)

                it["picked_news"] = picked

                self.stdout.write(
                    f"  - {scope} keyword='{kw}' candidates={len(candidates)}/{CANDIDATE_POOL_LIMIT} picked={len(picked)}/{NEWS_LIMIT}"
                )

            saved = _save_to_db(today, scope, items)
            self.stdout.write(
                self.style.SUCCESS(
                    f"[{today}] scope={scope} saved={saved} keywords with up to {NEWS_LIMIT} news each (content included, de-duplicated)."
                )
            )

        # âœ… KR/US ëª¨ë‘ ì €ì¥ ì™„ë£Œ í›„ ìë™ ë¶„ì„ ì‹¤í–‰
        self.stdout.write("=========================================")
        self.stdout.write("ğŸ” Auto-run: analyze_trend_keyword_news (pending only)")
        self.stdout.write("=========================================")

        try:
            call_command("analyze_trend_keyword_news")
            self.stdout.write(self.style.SUCCESS("âœ… Auto analysis finished: analyze_trend_keyword_news"))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"âŒ Auto analysis failed: {e}"))
